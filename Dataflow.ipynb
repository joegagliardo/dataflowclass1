{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b359750-81d1-48c1-a863-296635114ec2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Initialization Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036352e-57d4-4ff7-8155-60fb26e67a19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbca6a8-f8cc-45ac-8a62-0d14bfb665d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Initialize helper functions to run Java inside cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d334c3-e064-4241-aa70-26d939b3e166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> /opt/gradle-5.0/bin/gradle --console=plain -v\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-java.ipynb#scrollTo=CgTXBdTsBn1F\n",
    "# Run and print a shell command.\n",
    "def run(cmd, progress = True, verbose = False):\n",
    "  if progress:\n",
    "      print('>> {}'.format(cmd))\n",
    "    \n",
    "  if verbose:\n",
    "      !{cmd}  # This is magic to run 'cmd' in the shell.\n",
    "      print('')\n",
    "  else:\n",
    "      ! {cmd} > /dev/null 2>&1\n",
    "\n",
    "import os\n",
    "\n",
    "# Download the gradle source.\n",
    "gradle_version = 'gradle-5.0'\n",
    "gradle_path = f\"/opt/{gradle_version}\"\n",
    "if not os.path.exists(gradle_path):\n",
    "  run(f\"wget -q -nc -O gradle.zip https://services.gradle.org/distributions/{gradle_version}-bin.zip\")\n",
    "  run('unzip -q -d /opt gradle.zip')\n",
    "  run('rm -f gradle.zip')\n",
    "\n",
    "# We're choosing to use the absolute path instead of adding it to the $PATH environment variable.\n",
    "def gradle(args):\n",
    "  run(f\"{gradle_path}/bin/gradle --console=plain {args}\")\n",
    "\n",
    "gradle('-v')\n",
    "\n",
    "! mkdir -p src/main/java/samples/quickstart/\n",
    "print('Done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716a7d4-28ea-40e7-90db-3e762b7dbcf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definition for ___%%java___ Python magic cell function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a97925b-df54-4700-bb8b-d4a3e2305b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.magic import register_line_magic, register_cell_magic, register_line_cell_magic\n",
    "@register_cell_magic\n",
    "def java(line, cell):\n",
    "    \"\"\"\n",
    "    Written by Joseph Gagliardo Jr.\n",
    "    joegagliardo@gmail.com\n",
    "    2021-12-22\n",
    "    \"\"\"\n",
    "    gradle_text = \"\"\"\n",
    "plugins {\n",
    "  // id 'idea'     // Uncomment for IntelliJ IDE\n",
    "  // id 'eclipse'  // Uncomment for Eclipse IDE\n",
    "\n",
    "  // Apply java plugin and make it a runnable application.\n",
    "  id 'java'\n",
    "  id 'application'\n",
    "\n",
    "  // 'shadow' allows us to embed all the dependencies into a fat jar.\n",
    "  id 'com.github.johnrengelman.shadow' version '4.0.3'\n",
    "}\n",
    "\n",
    "// This is the path of the main class, stored within ./src/main/java/\n",
    "mainClassName = 'samples.quickstart.{class_name}'\n",
    "\n",
    "// Declare the sources from which to fetch dependencies.\n",
    "repositories {\n",
    "  mavenCentral()\n",
    "}\n",
    "\n",
    "// Java version compatibility.\n",
    "sourceCompatibility = 1.8\n",
    "targetCompatibility = 1.8\n",
    "\n",
    "// Use the latest Apache Beam major version 2.\n",
    "// You can also lock into a minor version like '2.9.+'.\n",
    "ext.apacheBeamVersion = '2.+'\n",
    "\n",
    "// Declare the dependencies of the project.\n",
    "dependencies {\n",
    "  shadow \"org.apache.beam:beam-sdks-java-core:$apacheBeamVersion\"\n",
    "\n",
    "  runtime \"org.apache.beam:beam-runners-direct-java:$apacheBeamVersion\"\n",
    "  compile \"org.apache.beam:beam-sdks-java-extensions-sql:$apacheBeamVersion\"\n",
    "  compile \"com.google.auto.value:auto-value-annotations:1.6\"\n",
    "  annotationProcessor \"com.google.auto.value:auto-value:1.6\"\n",
    "  runtime \"org.slf4j:slf4j-api:1.+\"\n",
    "  runtime \"org.slf4j:slf4j-jdk14:1.+\"\n",
    "\n",
    "  testCompile \"junit:junit:4.+\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "// Configure 'shadowJar' instead of 'jar' to set up the fat jar.\n",
    "shadowJar {\n",
    "  baseName = '{class_name}' // Name of the fat jar file.\n",
    "  classifier = null       // Set to null, otherwise 'shadow' appends a '-all' to the jar file name.\n",
    "  manifest {\n",
    "    attributes('Main-Class': mainClassName)  // Specify where the main class resides.\n",
    "  }\n",
    "}\n",
    "\"\"\"   \n",
    "    start = cell.find('class ')\n",
    "    end = cell.find(' {')\n",
    "    class_name = cell[start+6:end]\n",
    "    progress = 'noprogress' not in line.lower()\n",
    "    verbose = 'verbose' in line.lower()\n",
    "    output = 'nooutput' not in line.lower()\n",
    "\n",
    "    # if len(line) == 0:\n",
    "    #     start = cell.find('class ')\n",
    "    #     end = cell.find(' {')\n",
    "    #     class_name = cell[start+6:end]\n",
    "    # else:\n",
    "    #     class_name = line\n",
    "        \n",
    "    \n",
    "    run('rm src/main/java/samples/quickstart/*.java')\n",
    "    run('rm build/libs/*.jar')\n",
    "    run('rm -rf /tmp/outputs*', progress = progress, verbose = verbose)\n",
    "\n",
    "    with open('build.gradle', 'w') as f:\n",
    "        f.write(gradle_text.replace('{class_name}', class_name))\n",
    "\n",
    "    with open(f'src/main/java/samples/quickstart/{class_name}.java', 'w') as f:\n",
    "        f.write(cell)\n",
    "        \n",
    "    # Build the project.\n",
    "    run(f\"{gradle_path}/bin/gradle --console=plain build\", progress = progress, verbose = verbose)\n",
    "    run('ls -lh build/libs/', progress = progress, verbose = verbose)\n",
    "    run(f\"{gradle_path}/bin/gradle --console=plain runShadow\", progress = progress, verbose = verbose)\n",
    "    # run('head -n 20 /tmp/outputs*')\n",
    "    if output:\n",
    "        run('cat /tmp/outputs*', progress = False, verbose = True)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3957ba-068f-4325-827a-3b21aafeb702",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5ce6c-55cb-45b7-9ad1-370ee54bf5c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Install a Spark docker using the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3835ac48-b14e-4e01-8290-b9799ba6e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Using default tag: latest\n",
      "latest: Pulling from bitnami/spark\n",
      "Digest: sha256:ba1b7f08660ffaa48a98abf71de712c89b92b194f9efaef4a8af4575c1809af9\n",
      "Status: Image is up to date for bitnami/spark:latest\n",
      "docker.io/bitnami/spark:latest\n",
      "Error response from daemon: network with name spark_network already exists\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "ln: failed to create symbolic link '/opt/conda/lib/libtinfor.so.6': File exists\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "! docker pull bitnami/spark && \\\n",
    "docker network create spark_network && \\\n",
    "docker run -d --name spark --network=spark_network -e SPARK_MODE=master bitnami/spark\n",
    "! ln -s /opt/conda/lib/libtinfo.so /opt/conda/lib/libtinfor.so.6\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659dd20-a9ab-4008-8bce-a5ff5acf5690",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Install pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fdb46e-7eef-4af7-bd41-587e716e3be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /root/apache-beam-2.34.0/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /root/apache-beam-2.34.0/lib/python3.7/site-packages (from pyspark) (0.10.9.2)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "\n",
    "def install(package):\n",
    "    if hasattr(pip, 'main'):\n",
    "        pip.main(['install', package])\n",
    "    else:\n",
    "        pip._internal.main(['install', package])\n",
    "\n",
    "install('pyspark')\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2140ae18-4293-4560-91d6-4324e41fb0b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Initialize the Spark context variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573258c-97d5-4282-9278-cfe9ad82a186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def initspark(appname = \"Notebook\", servername = \"local[*]\"):\n",
    "    print ('initializing pyspark')\n",
    "    conf = SparkConf().setAppName(appname).setMaster(servername)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession.builder.appName(appname).enableHiveSupport().getOrCreate()\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    print ('pyspark initialized')\n",
    "    return sc, spark, conf\n",
    "\n",
    "sc, spark, conf = initspark()\n",
    "print(sc, spark)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2bd3e-f247-4edc-87d1-27aeb165ecc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20303e33-eb39-459b-85cf-743209e5322d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbf532-1859-4134-a64f-63b6f14f4268",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. ___Create___ allows you to upload data into a PCollection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f8b59-343b-45c9-b02d-7399bceefbe0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1397c54-3ff7-4188-bc9e-c72bef0d2670",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Non Beam example of applying a map function to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0dd11-7c70-4a35-bc61-51d36bcfaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['one', 'two', 'three', 'four']\n",
    "print(list(map(str.title, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb1aad-7fe0-4b99-96f4-b30fbaae23ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simple transformation, turn the local collection into a PCollection and apply a Map PTransform on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedc12b-27d2-4098-85f0-feba2f9e14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | beam.Map(str.title)\n",
    "          | beam.Map(print)\n",
    "    )\n",
    "\n",
    "# lines is a PCollection object\n",
    "print('lines = ', lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c8ecc-a678-48eb-9ae8-7fd3e34b8a6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simple transformation using a lambda instead of a built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9b54f-3a72-4688-a5d1-545077e6b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | beam.Map(lambda x : x.title())\n",
    "          | beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977dfeee-d660-47fb-92f7-7a0c3bcbb302",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simple transformation using a user defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4cad9-4ed9-4c26-8894-e7d84eff8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def title(x):\n",
    "    return x.title()\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | beam.Map(title)\n",
    "          | beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f09af7-2678-4b0c-879e-c567625a4a83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Python the pipe `|` is actually just an operator overload to call the apply method of the pipeline. You would never do this in python, but it helps to understand what is going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e7a3d-99fc-4904-a1cd-baf93d088b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        lines = ((p | beam.Create(['one', 'two', 'three', 'four']))\n",
    "             .apply(beam.Map(str.title)) \n",
    "             .apply(beam.Map(print))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b057071-63e3-4414-820d-71e07ed0959d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Spark equivalent would be to pload a local Python list into a Spark RDD and do a simple transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b110e0-45a0-476b-a2ca-d33cecaec241",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = ( sc.parallelize(['one', 'two', 'three', 'four'])\n",
    "        \n",
    "#           .map(str.title)\n",
    "       )\n",
    "rdd1.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebfb43-8dd1-491e-9422-350ecda44d1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33b715-0a74-4b46-8736-7798c51b313a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple transformation using a ___lambda___.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8420314f-13dd-410f-90a8-569c9f3770ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rm src/main/java/samples/quickstart/*.java\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm build/libs/*.jar\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm -rf /tmp/outputs*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes\n",
      "> Task :jar\n",
      "> Task :startScripts UP-TO-DATE\n",
      "> Task :distTar\n",
      "> Task :distZip\n",
      "> Task :shadowJar\n",
      "> Task :startShadowScripts\n",
      "> Task :shadowDistTar\n",
      "> Task :shadowDistZip\n",
      "> Task :assemble\n",
      "> Task :compileTestJava NO-SOURCE\n",
      "> Task :processTestResources NO-SOURCE\n",
      "> Task :testClasses UP-TO-DATE\n",
      "> Task :test NO-SOURCE\n",
      "> Task :check UP-TO-DATE\n",
      "> Task :build\n",
      "\n",
      "BUILD SUCCESSFUL in 25s\n",
      "9 actionable tasks: 8 executed, 1 up-to-date\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 68M\n",
      "-rw-r--r-- 1 root root  68M Dec 28 01:52 Create1.jar\n",
      "-rw-r--r-- 1 root root 2.2K Dec 28 01:51 dataflowclass1.jar\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava UP-TO-DATE\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes UP-TO-DATE\n",
      "> Task :shadowJar UP-TO-DATE\n",
      "> Task :startShadowScripts UP-TO-DATE\n",
      "> Task :installShadowDist\n",
      "\n",
      "> Task :runShadow\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 0e1fc1a6-1a0e-4fff-aef7-c143063be2de for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@5c86a017 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer e6f84f29-ccf3-41a1-9217-b2fc2c1dccda for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@5c86a017 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 265841d6-da66-4f55-a836-4e5a031951ab for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@5c86a017 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/e6f84f29-ccf3-41a1-9217-b2fc2c1dccda\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/0e1fc1a6-1a0e-4fff-aef7-c143063be2de\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/265841d6-da66-4f55-a836-4e5a031951ab\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.WriteFiles$FinalizeTempFileBundles$FinalizeFn process\n",
      "INFO: Finalizing 3 file results\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Finalizing for destination null num shards 3.\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/0e1fc1a6-1a0e-4fff-aef7-c143063be2de, shard=2, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@5c86a017, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /tmp/outputs-00002-of-00003\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/e6f84f29-ccf3-41a1-9217-b2fc2c1dccda, shard=0, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@5c86a017, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /tmp/outputs-00000-of-00003\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/265841d6-da66-4f55-a836-4e5a031951ab, shard=1, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@5c86a017, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /tmp/outputs-00001-of-00003\n",
      "Dec 28, 2021 1:52:18 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation removeTemporaryFiles\n",
      "WARNING: Failed to match temporary files under: [/tmp/.temp-beam-e49a2a42-4999-46ed-bb52-2a8ac39da3c5/].\n",
      "\n",
      "BUILD SUCCESSFUL in 6s\n",
      "5 actionable tasks: 2 executed, 3 up-to-date\n",
      "\u001b[m\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "TWO\n",
      "FOUR\n",
      "ONE\n",
      "THREE\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java verbose\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "\n",
    "import java.util.*;\n",
    "\n",
    "public class Create1 {\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "        Pipeline p = Pipeline.create();\n",
    "        \n",
    "        PCollection<String> lines = p.apply(Create.of(\"one\", \"two\", \"three\", \"four\"));\n",
    "        lines = lines.apply(MapElements.into(TypeDescriptors.strings()).via((String line) -> line.toUpperCase()));\n",
    "        lines.apply(TextIO.write().to(outputsPrefix));\n",
    "\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3db7b6-c42d-4268-847c-24a9c39df7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple transformation using ___SimpleFunction___ instead of lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ff75092-0480-4784-afba-9616e9e60d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rm src/main/java/samples/quickstart/*.java\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm build/libs/*.jar\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm -rf /tmp/outputs*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes\n",
      "> Task :jar\n",
      "> Task :startScripts UP-TO-DATE\n",
      "> Task :distTar\n",
      "> Task :distZip\n",
      "> Task :shadowJar\n",
      "> Task :startShadowScripts\n",
      "> Task :shadowDistTar\n",
      "> Task :shadowDistZip\n",
      "> Task :assemble\n",
      "> Task :compileTestJava NO-SOURCE\n",
      "> Task :processTestResources NO-SOURCE\n",
      "> Task :testClasses UP-TO-DATE\n",
      "> Task :test NO-SOURCE\n",
      "> Task :check UP-TO-DATE\n",
      "> Task :build\n",
      "\n",
      "BUILD SUCCESSFUL in 25s\n",
      "9 actionable tasks: 8 executed, 1 up-to-date\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 68M\n",
      "-rw-r--r-- 1 root root  68M Dec 28 01:57 Create2.jar\n",
      "-rw-r--r-- 1 root root 2.3K Dec 28 01:57 dataflowclass1.jar\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava UP-TO-DATE\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes UP-TO-DATE\n",
      "> Task :shadowJar UP-TO-DATE\n",
      "> Task :startShadowScripts UP-TO-DATE\n",
      "> Task :installShadowDist\n",
      "\n",
      "> Task :runShadow\n",
      "** THREE\n",
      "** TWO\n",
      "** ONE\n",
      "** FOUR\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 7c894849-e316-45cb-875e-737529685955 for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@6a4f1a55 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer ed74178e-63a9-4fd1-b5e0-60d44a9388eb for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@6a4f1a55 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 17fc935c-6ab4-48dd-a0fd-1c930b683e72 for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@6a4f1a55 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/17fc935c-6ab4-48dd-a0fd-1c930b683e72\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/7c894849-e316-45cb-875e-737529685955\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/ed74178e-63a9-4fd1-b5e0-60d44a9388eb\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.WriteFiles$FinalizeTempFileBundles$FinalizeFn process\n",
      "INFO: Finalizing 3 file results\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Finalizing for destination null num shards 3.\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/17fc935c-6ab4-48dd-a0fd-1c930b683e72, shard=0, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@6a4f1a55, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /tmp/outputs-00000-of-00003\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/7c894849-e316-45cb-875e-737529685955, shard=1, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@6a4f1a55, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /tmp/outputs-00001-of-00003\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/ed74178e-63a9-4fd1-b5e0-60d44a9388eb, shard=2, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@6a4f1a55, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /tmp/outputs-00002-of-00003\n",
      "Dec 28, 2021 1:58:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation removeTemporaryFiles\n",
      "WARNING: Failed to match temporary files under: [/tmp/.temp-beam-7bb52895-6db8-47e4-87a0-d5c857eeee0b/].\n",
      "\n",
      "BUILD SUCCESSFUL in 6s\n",
      "5 actionable tasks: 2 executed, 3 up-to-date\n",
      "\u001b[m\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "ONE\n",
      "TWO\n",
      "THREE\n",
      "FOUR\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java verbose\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import java.util.*;\n",
    "\n",
    "public class Create2 {\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "        Pipeline p = Pipeline.create();\n",
    "        \n",
    "        PCollection<String> lines = p.apply(Create.of(\"one\", \"two\", \"three\", \"four\"));\n",
    "        lines = lines.apply(MapElements.via(\n",
    "            new SimpleFunction<String, String>() {\n",
    "              @Override\n",
    "              public String apply(String line) {\n",
    "                String ret = line.toUpperCase();\n",
    "                //System.out.println(\"** \" + ret);\n",
    "                return ret;\n",
    "              }\n",
    "            }));\n",
    "\n",
    "        lines.apply(\"Write\", TextIO.write().to(outputsPrefix));\n",
    "\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f27aee-6b1c-44a5-815d-31d039c24475",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Java simple transformation using ___SimpleFunction___ to wrap a User Defined Function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c178e-2cfa-4316-95de-5779b243f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import java.util.*;\n",
    "\n",
    "public class Create3 {\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "        Pipeline p = Pipeline.create();\n",
    "        \n",
    "        PCollection<String> lines = p.apply(Create.of(\"one\", \"two\", \"three\", \"four\"));\n",
    "        lines = lines.apply(MapElements.via(\n",
    "            new SimpleFunction<String, String>() {\n",
    "              @Override\n",
    "              public String apply(String line) {\n",
    "                return upper(line);\n",
    "              }\n",
    "            }));\n",
    "\n",
    "        lines.apply(\"Write\", TextIO.write().to(outputsPrefix));\n",
    "\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    public static String upper(String line) {\n",
    "        return line.toUpperCase();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5781d-6836-44ce-bca3-34585c5e352f",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591af669-d68f-471e-a03a-82dbaaa7a609",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931beffe-4cf4-48e4-a28d-8a2dfaed62c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. ___ReadFromText___ allows you to read a text file into a __PCollection__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db17f3-ea3a-4418-b12e-5b859d1e9856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab382f85-16e9-4544-a56b-06c6d4edc7b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### It's a good idea to start naming the steps for debugging and monitoring later. Names must be unique in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53066990-43fc-42c0-9545-ddf723145fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/outputs*\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.Map(lambda x : x.split(','))\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "          | 'Write' >> WriteToText('/tmp/outputs')\n",
    "#          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run() # implicit in Python when using with block\n",
    "\n",
    "! cat /tmp/outputs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88479c-60c3-48c3-b8c1-3e8113f0d39a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Read from CSV and use ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94887a8a-f945-414f-bfaa-2e72fa1aa746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "#        yield (int(regionid), regionname.upper()) # Include a transformation instead of doing it as a separate step\n",
    "\n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "          #| 'Write' >> WriteToText('regions.out')\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3fd79-61c1-4d9f-acf1-5ee7af3ff832",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Java\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d05cc4-e81e-44d7-adad-d38ee19848c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Read from CSV and use Map with ___lambda___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28492eea-c43a-43ab-aa7f-ceca3f199a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "\n",
    "public class ReadRegions1 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", MapElements.into(TypeDescriptors.strings()).via((String element) -> element.toUpperCase()));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbd1da-0fed-43c9-8ab1-e1b73cc9534f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ___ParDo___ Example using anonymous class inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14327a97-f8cd-489b-87eb-82ad962f701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "\n",
    "public class ReadRegions2 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new DoFn<String, String>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    String element = c.element();\n",
    "                    // String[] elements = element.split(\",\");\n",
    "                    c.output(element + \"*\");\n",
    "                }\n",
    "            }));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc4515-7a9b-40f9-99cf-c456065c3e11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ___ParDo___ using a defined class instead of an anonynous class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8543a5-9d2e-4a6e-81f4-5ae59aedcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "\n",
    "public class ReadRegions3 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new AddStar()));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    static class AddStar extends DoFn<String, String> {\n",
    "        @ProcessElement\n",
    "        public void process(@Element String line, OutputReceiver<String> out) {\n",
    "            out.output(line + \"*\");\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ce401-ba9d-4fd8-a21c-670049b2a716",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889a629-d0e8-4cbc-9673-840de48cbf00",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cae856-d38b-4ce9-a4c1-511284e1f9bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. Parse into a model class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4b34a-2b4a-4f8f-aa9c-af369f7ef9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebb6b4-3c55-4b79-a0d8-4d3c017162c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a model based on ___typing.NamedTuple___ so you can use properties instead of keys and use the Filter __PTransform__ with ___lambda___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0dd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x.regionid % 2 == 0)\n",
    "          | 'Filter 2' >> beam.Filter(lambda x : x.territoryname.startswith('S'))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230451d-cfb3-450f-82b6-e5dd24e6e2e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use Filter with a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07089b39-7ccf-42cf-830b-3189ab269566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "def startsWithS(element):\n",
    "    return element.territoryname.startswith('S')\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter' >> beam.Filter(startsWithS)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac3ac5-d61b-496d-bf95-2801b014b28e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use a ParDo class to accomplish filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691011-9bfd-49d9-8992-ff1333c65824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element.territoryname.startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter' >> beam.ParDo(StartsWithSFilter())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39807dbe-b99a-4767-be21-16b38ff287f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Put the parsing and filtering all into one ParDo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2091357-5550-42fb-8810-3e5fc8d2cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        if territoryname.startswith('S'):\n",
    "            yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7777973-1216-4e12-995f-5c5a8247b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  # records = (p | 'Read' >> beam.io.ReadFromAvro('gs://joey-shared-bucket/datasets/northwind/AVRO/categories/categories.avro')\n",
    "  records = (p | 'Read' >> beam.io.ReadFromText('gs://joey-shared-bucket/datasets/northwind/CSV/categories/categories.csv')\n",
    "             | beam.Map(print))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb0592-e4b0-4204-b4f0-775e75d8f02c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e66252-cdcd-411c-866e-44857a6b4379",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parse a CSV into a class and filter it using a Pardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1e99c-eece-43f4-9b8e-2ba9df28d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"Filter\", ParDo.of(new FilterTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    static class FilterTerritories extends DoFn<Territory, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(FilterTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(@Element Territory t, OutputReceiver<Territory> o) {\n",
    "            if (t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\")) {\n",
    "                o.output(t);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378c125-c36c-4ac0-9443-35f54539258a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parse a CSV into a class and filter it using and anonymous class to create the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292cee7-d525-44b6-a9db-36b5f3acf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"Filter\", Filter.by(new SerializableFunction<Territory, Boolean>() {\n",
    "                @Override\n",
    "                public Boolean apply(Territory t) {\n",
    "                    return t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\");\n",
    "                }\n",
    "            }))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    static class FilterTerritories extends DoFn<Territory, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(FilterTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(@Element Territory t, OutputReceiver<Territory> o) {\n",
    "            if (t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\")) {\n",
    "                o.output(t);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2fc88-b027-42bf-9136-c02d6583a04c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parse a CSV into a class and filter it in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339e563-880b-4fae-9cc6-3e8ba50d85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                if (territoryName.startsWith(\"S\")) {\n",
    "                    c.output(new Territory(territoryID, territoryName, regionID));\n",
    "                }\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    static class FilterTerritories extends DoFn<Territory, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(FilterTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(@Element Territory t, OutputReceiver<Territory> o) {\n",
    "            if (t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\")) {\n",
    "                o.output(t);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505552a2-9b45-48b2-a0b1-5165573d33bf",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72891e0-232c-4a11-a7ec-1880c8c71425",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5c5a3-327e-4279-af35-bcf57e7ab9f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Create multiple outputs from a single read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc9883-ba3b-4b67-ad1c-05b281ef9474",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edcd4b-aac7-412a-a728-d772fd625f56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Send the same data down multiple paths, such as to group it on two different keys with one read from the source. Also show how to read AVRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield Territory(int(element['territoryid']), element['territorydescription'], int(element['regionid']))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/AVRO/territories/territories.avro'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (p | 'Read' >> beam.io.ReadFromAvro(territoriesfilename)\n",
    "                     | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "                  )\n",
    "\n",
    "    # Branch 1\n",
    "    (territories \n",
    "         | 'Lowercase' >> beam.Map(lambda x : (x.territoryid, x.territoryname.lower(), x.regionid))\n",
    "         | 'Write Lower' >> WriteToText('/tmp/territories_lower.out')\n",
    "    )\n",
    "    \n",
    "    # Branch 2\n",
    "    (territories \n",
    "         | 'Uppercase' >> beam.Map(lambda x : (x.territoryid, x.territoryname.upper(), x.regionid))\n",
    "         | 'Write Upper' >> WriteToText('/tmp/territories_upper.out')\n",
    "    )\n",
    "\n",
    "! echo \"Lower\" && cat /tmp/territories_lower.out* && echo \"Upper\" && cat /tmp/territories_upper.out*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7374d-4d8b-4923-81b6-4c32e9e13c99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use TaggedOutput in the ParDo to split data into two different paths with different data on each. Also show how to read Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d221e6-669a-40eb-b2bc-7cdcf693ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/PARQUET/territories/territories.parquet'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  records = p | 'Read' >> beam.io.ReadFromParquet(territoriesfilename) | beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7649fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class OddEvenTerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = int(element['territoryid']), element['territoryname'], int(element['regionid'])\n",
    "        if int(regionid) % 2 == 0:\n",
    "            yield pvalue.TaggedOutput('Even', Territory(int(territoryid), territoryname, int(regionid)))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput('Odd', Territory(int(territoryid), territoryname, int(regionid)))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/PARQUET/territories/territories.parquet'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories = p | 'Read' >> beam.io.ReadFromParquet(territoriesfilename) \n",
    "    # territories would return a tuple of the two tagged outputs\n",
    "    # unpack the two outputs to two separate variables to process differently\n",
    "    evens, odds = territories | 'Parse' >> beam.ParDo(OddEvenTerritoryParseClass()).with_outputs(\"Even\", \"Odd\")\n",
    "    \n",
    "    evens | 'Write Even' >> WriteToText('/tmp/territories_even.out')\n",
    "    \n",
    "    odds | 'Write Odd' >> WriteToText('/tmp/territories_odd.out')\n",
    "\n",
    "! echo \"Evens\" && cat /tmp/territories_even.out* && echo \"Odds\" && cat /tmp/territories_odd.out*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a3ae2-0add-4eaf-8515-d9420834fa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ce4a7-eacc-41dd-b51b-1746546fcbcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Send the same output down two different paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43ccc6-3890-4349-8482-c7c2498e9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/territories*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9db37-f11f-491a-b34c-6a52cebb5cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Incomplete AVRO example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4d2295-86ea-4a50-a094-d4d8eaff9145",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    return pipeline.run();\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "%%java nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTagList;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    " Pipeline p = ...;\n",
    "\n",
    "#  // Read Avro-generated classes from files on GCS\n",
    "#  PCollection<AvroAutoGenClass> records =\n",
    "#      p.apply(AvroIO.read(AvroAutoGenClass.class).from(\"gs://my_bucket/path/to/records-*.avro\"));\n",
    "\n",
    "#  // Read GenericRecord's of the given schema from files on GCS\n",
    "#  Schema schema = new Schema.Parser().parse(new File(\"schema.avsc\"));\n",
    "#  PCollection<GenericRecord> records =\n",
    "#      p.apply(AvroIO.readGenericRecords(schema)\n",
    "#                 .from(\"gs://my_bucket/path/to/records-*.avro\"));\n",
    " \n",
    "    \n",
    "  pipeline.apply(\"Read Avro files\",\n",
    "      AvroIO.readGenericRecords(schemaJson).from(options.getInputFile()))\n",
    "      .apply(\"Convert Avro to CSV formatted data\",\n",
    "          ParDo.of(new ConvertAvroToCsv(schemaJson, options.getCsvDelimiter())))\n",
    "      .apply(\"Write CSV formatted data\", TextIO.write().to(options.getOutput())\n",
    "          .withSuffix(\".csv\"));\n",
    "    \n",
    "    \n",
    "    \n",
    "pipeline\n",
    "      .apply(\"Read from Avro\", AvroIO.read(BigtableRow.class).from(options.getInputFilePattern()))\n",
    "      .apply(\n",
    "          \"Transform to Bigtable\",\n",
    "          ParDo.of(\n",
    "              AvroToBigtableFn.createWithSplitLargeRows(\n",
    "                  options.getSplitLargeRows(), MAX_MUTATIONS_PER_ROW)))\n",
    "      .apply(\"Write to Bigtable\", write);\n",
    "\n",
    "  return pipeline.run();\n",
    "\n",
    "\n",
    "\n",
    "public class ReadTerritoriesAvro {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/AVRO/territories/territories.avro\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<AvroAutoGenClass> records= p\n",
    "            .apply(\"Read Avro\", AvroIO.read(AvroAutoGenClass.class).from(territoriesInputFileName));\n",
    "/*\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse Territory\", ParDo.of(new ParseTerritories()))\n",
    "*/\n",
    "        ;                   \n",
    "        \n",
    "/*            \n",
    "        territories\n",
    "            .apply(\"Upper\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toUpperCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_upper\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        territories\n",
    "            .apply(\"Lower\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toLowerCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_lower\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritoriesOddEvenSplit: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    */\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c6953-65a2-4cbd-8f77-92c6f87cfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReference tableRef = new TableReference();\n",
    "tableRef.setProjectId(\"project-id\");\n",
    "tableRef.setDatasetId(\"dataset-name\");\n",
    "tableRef.setTableId(\"table-name\");\n",
    "\n",
    "List<TableFieldSchema> fieldDefs = new ArrayList<>();\n",
    "fieldDefs.add(new TableFieldSchema().setName(\"column1\").setType(\"STRING\"));\n",
    "fieldDefs.add(new TableFieldSchema().setName(\"column2\").setType(\"FLOAT\"));  \n",
    "For the Pipeline steps,\n",
    "\n",
    "Pipeline pipeLine = Pipeline.create(options);\n",
    "pipeLine\n",
    ".apply(\"ReadMyFile\", \n",
    "        TextIO.read().from(\"path-to-json-file\")) \n",
    "\n",
    ".apply(\"MapToTableRow\", ParDo.of(new DoFn<String, TableRow>() {\n",
    "    @ProcessElement\n",
    "    public void processElement(ProcessContext c) { \n",
    "        Gson gson = new GsonBuilder().create();\n",
    "        HashMap<String, Object> parsedMap = gson.fromJson(c.element().toString(), HashMap.class);\n",
    "\n",
    "        TableRow row = new TableRow();\n",
    "        row.set(\"column1\", parsedMap.get(\"col1\").toString());\n",
    "        row.set(\"column2\", Double.parseDouble(parsedMap.get(\"col2\").toString()));\n",
    "        c.output(row);\n",
    "    }\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e53ead-defb-46a7-9710-77616520f9d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9847873-48ba-4b10-9e17-9bfe3004ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rm src/main/java/samples/quickstart/*.java\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm build/libs/*.jar\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm -rf /tmp/outputs*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTagList;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse Territory\", ParDo.of(new ParseTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "            \n",
    "        territories\n",
    "            .apply(\"Upper\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toUpperCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_upper\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        territories\n",
    "            .apply(\"Lower\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toLowerCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_lower\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritoriesOddEvenSplit: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae170dd2-fdd2-46fc-b741-5e8423269d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Upper\n",
      "(territoryID = 85014, territoryName = PHOENIX, regionID = 2)\n",
      "(territoryID = 94105, territoryName = SAN FRANCISCO, regionID = 2)\n",
      "(territoryID = 98004, territoryName = BELLEVUE, regionID = 2)\n",
      "(territoryID = 1730, territoryName = BEDFORD, regionID = 1)\n",
      "(territoryID = 2184, territoryName = BRAINTREE, regionID = 1)\n",
      "(territoryID = 6897, territoryName = WILTON, regionID = 1)\n",
      "(territoryID = 10038, territoryName = NEW YORK, regionID = 1)\n",
      "(territoryID = 48084, territoryName = TROY, regionID = 3)\n",
      "(territoryID = 55439, territoryName = MINNEAPOLIS, regionID = 3)\n",
      "(territoryID = 78759, territoryName = AUSTIN, regionID = 4)\n",
      "(territoryID = 11747, territoryName = MELLVILE, regionID = 1)\n",
      "(territoryID = 20852, territoryName = ROCKVILLE, regionID = 1)\n",
      "(territoryID = 30346, territoryName = ATLANTA, regionID = 4)\n",
      "(territoryID = 40222, territoryName = LOUISVILLE, regionID = 1)\n",
      "(territoryID = 85251, territoryName = SCOTTSDALE, regionID = 2)\n",
      "(territoryID = 95008, territoryName = CAMPBELL, regionID = 2)\n",
      "(territoryID = 98052, territoryName = REDMOND, regionID = 2)\n",
      "(territoryID = 1833, territoryName = GEORGETOW, regionID = 1)\n",
      "(territoryID = 2903, territoryName = PROVIDENCE, regionID = 1)\n",
      "(territoryID = 7960, territoryName = MORRISTOWN, regionID = 1)\n",
      "(territoryID = 48304, territoryName = BLOOMFIELD HILLS, regionID = 3)\n",
      "(territoryID = 60179, territoryName = HOFFMAN ESTATES, regionID = 2)\n",
      "(territoryID = 80202, territoryName = DENVER, regionID = 2)\n",
      "(territoryID = 14450, territoryName = FAIRPORT, regionID = 1)\n",
      "(territoryID = 27403, territoryName = GREENSBORO, regionID = 1)\n",
      "(territoryID = 31406, territoryName = SAVANNAH, regionID = 4)\n",
      "(territoryID = 44122, territoryName = BEACHWOOD, regionID = 3)\n",
      "(territoryID = 90405, territoryName = SANTA MONICA, regionID = 2)\n",
      "(territoryID = 95054, territoryName = SANTA CLARA, regionID = 2)\n",
      "(territoryID = 98104, territoryName = SEATTLE, regionID = 2)\n",
      "(territoryID = 2116, territoryName = BOSTON, regionID = 1)\n",
      "(territoryID = 3049, territoryName = HOLLIS, regionID = 3)\n",
      "(territoryID = 8837, territoryName = EDISON, regionID = 1)\n",
      "(territoryID = 60601, territoryName = CHICAGO, regionID = 2)\n",
      "(territoryID = 19428, territoryName = PHILADELPHIA, regionID = 3)\n",
      "(territoryID = 27511, territoryName = CARY, regionID = 1)\n",
      "(territoryID = 32859, territoryName = ORLANDO, regionID = 4)\n",
      "(territoryID = 45839, territoryName = FINDLAY, regionID = 3)\n",
      "(territoryID = 53404, territoryName = RACINE, regionID = 3)\n",
      "(territoryID = 80909, territoryName = COLORADO SPRINGS, regionID = 2)\n",
      "(territoryID = 94025, territoryName = MENLO PARK, regionID = 2)\n",
      "(territoryID = 95060, territoryName = SANTA CRUZ, regionID = 2)\n",
      "(territoryID = 1581, territoryName = WESTBORO, regionID = 1)\n",
      "(territoryID = 2139, territoryName = CAMBRIDGE, regionID = 1)\n",
      "(territoryID = 3801, territoryName = PORTSMOUTH, regionID = 3)\n",
      "(territoryID = 10019, territoryName = NEW YORK, regionID = 1)\n",
      "(territoryID = 48075, territoryName = SOUTHFIELD, regionID = 3)\n",
      "(territoryID = 55113, territoryName = ROSEVILLE, regionID = 3)\n",
      "(territoryID = 75234, territoryName = DALLAS, regionID = 4)\n",
      "(territoryID = 19713, territoryName = NEWARD, regionID = 1)\n",
      "(territoryID = 29202, territoryName = COLUMBIA, regionID = 4)\n",
      "(territoryID = 33607, territoryName = TAMPA, regionID = 4)\n",
      "(territoryID = 72716, territoryName = BENTONVILLE, regionID = 4)\n",
      "Lower\n",
      "(territoryID = 78759, territoryName = austin, regionID = 4)\n",
      "(territoryID = 48084, territoryName = troy, regionID = 3)\n",
      "(territoryID = 80202, territoryName = denver, regionID = 2)\n",
      "(territoryID = 94025, territoryName = menlo park, regionID = 2)\n",
      "(territoryID = 98004, territoryName = bellevue, regionID = 2)\n",
      "(territoryID = 11747, territoryName = mellvile, regionID = 1)\n",
      "(territoryID = 27403, territoryName = greensboro, regionID = 1)\n",
      "(territoryID = 32859, territoryName = orlando, regionID = 4)\n",
      "(territoryID = 1730, territoryName = bedford, regionID = 1)\n",
      "(territoryID = 2903, territoryName = providence, regionID = 1)\n",
      "(territoryID = 8837, territoryName = edison, regionID = 1)\n",
      "(territoryID = 48304, territoryName = bloomfield hills, regionID = 3)\n",
      "(territoryID = 80909, territoryName = colorado springs, regionID = 2)\n",
      "(territoryID = 94105, territoryName = san francisco, regionID = 2)\n",
      "(territoryID = 98052, territoryName = redmond, regionID = 2)\n",
      "(territoryID = 60179, territoryName = hoffman estates, regionID = 2)\n",
      "(territoryID = 14450, territoryName = fairport, regionID = 1)\n",
      "(territoryID = 27511, territoryName = cary, regionID = 1)\n",
      "(territoryID = 33607, territoryName = tampa, regionID = 4)\n",
      "(territoryID = 1833, territoryName = georgetow, regionID = 1)\n",
      "(territoryID = 3049, territoryName = hollis, regionID = 3)\n",
      "(territoryID = 10019, territoryName = new york, regionID = 1)\n",
      "(territoryID = 53404, territoryName = racine, regionID = 3)\n",
      "(territoryID = 85014, territoryName = phoenix, regionID = 2)\n",
      "(territoryID = 95008, territoryName = campbell, regionID = 2)\n",
      "(territoryID = 98104, territoryName = seattle, regionID = 2)\n",
      "(territoryID = 60601, territoryName = chicago, regionID = 2)\n",
      "(territoryID = 19428, territoryName = philadelphia, regionID = 3)\n",
      "(territoryID = 29202, territoryName = columbia, regionID = 4)\n",
      "(territoryID = 40222, territoryName = louisville, regionID = 1)\n",
      "(territoryID = 2116, territoryName = boston, regionID = 1)\n",
      "(territoryID = 3801, territoryName = portsmouth, regionID = 3)\n",
      "(territoryID = 10038, territoryName = new york, regionID = 1)\n",
      "(territoryID = 45839, territoryName = findlay, regionID = 3)\n",
      "(territoryID = 55113, territoryName = roseville, regionID = 3)\n",
      "(territoryID = 85251, territoryName = scottsdale, regionID = 2)\n",
      "(territoryID = 95054, territoryName = santa clara, regionID = 2)\n",
      "(territoryID = 72716, territoryName = bentonville, regionID = 4)\n",
      "(territoryID = 19713, territoryName = neward, regionID = 1)\n",
      "(territoryID = 30346, territoryName = atlanta, regionID = 4)\n",
      "(territoryID = 44122, territoryName = beachwood, regionID = 3)\n",
      "(territoryID = 2139, territoryName = cambridge, regionID = 1)\n",
      "(territoryID = 6897, territoryName = wilton, regionID = 1)\n",
      "(territoryID = 75234, territoryName = dallas, regionID = 4)\n",
      "(territoryID = 48075, territoryName = southfield, regionID = 3)\n",
      "(territoryID = 55439, territoryName = minneapolis, regionID = 3)\n",
      "(territoryID = 90405, territoryName = santa monica, regionID = 2)\n",
      "(territoryID = 95060, territoryName = santa cruz, regionID = 2)\n",
      "(territoryID = 20852, territoryName = rockville, regionID = 1)\n",
      "(territoryID = 31406, territoryName = savannah, regionID = 4)\n",
      "(territoryID = 1581, territoryName = westboro, regionID = 1)\n",
      "(territoryID = 2184, territoryName = braintree, regionID = 1)\n",
      "(territoryID = 7960, territoryName = morristown, regionID = 1)\n"
     ]
    }
   ],
   "source": [
    "! echo \"Upper\" && cat /tmp/territories_upper* && echo \"Lower\" && cat /tmp/territories_lower*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c0f98-4b4b-4253-8045-4bb46bebfa7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use TupleTag to split the output into two separate path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25158a5b-169d-4672-9683-38732dbca1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/territories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72dde632-b934-4bf6-9a58-b551ab264385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rm src/main/java/samples/quickstart/*.java\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm build/libs/*.jar\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm -rf /tmp/outputs*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTagList;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "\n",
    "    final static TupleTag<Territory> evenTag = new TupleTag<Territory>() {};\n",
    "    final static TupleTag<Territory> oddTag = new TupleTag<Territory>() {};\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollectionTuple territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"OddEvenSplit\", ParDo.of(new ParseTerritoriesOddEvenSplit()).withOutputTags(evenTag, TupleTagList.of(oddTag)))\n",
    "        ;                   \n",
    "        \n",
    "        PCollection<Territory> evenTerritories = territories.get(evenTag);\n",
    "        evenTerritories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix + \"_even\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        PCollection<Territory> oddTerritories = territories.get(oddTag);\n",
    "        oddTerritories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix + \"_odd\").withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritoriesOddEvenSplit extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritoriesOddEvenSplit.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "\n",
    "\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                if (regionID % 2 == 0) {\n",
    "                    c.output(evenTag, new Territory(territoryID, territoryName, regionID));\n",
    "                } else {\n",
    "                    c.output(oddTag, new Territory(territoryID, territoryName, regionID));\n",
    "                }\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritoriesOddEvenSplit: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6d6ae9-a392-4d7f-9222-017e3f2a8d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Odd\n",
      "(territoryID = 48304, territoryName = Bloomfield Hills, regionID = 3)\n",
      "(territoryID = 1730, territoryName = Bedford, regionID = 1)\n",
      "(territoryID = 2184, territoryName = Braintree, regionID = 1)\n",
      "(territoryID = 7960, territoryName = Morristown, regionID = 1)\n",
      "(territoryID = 14450, territoryName = Fairport, regionID = 1)\n",
      "(territoryID = 27511, territoryName = Cary, regionID = 1)\n",
      "(territoryID = 53404, territoryName = Racine, regionID = 3)\n",
      "(territoryID = 1581, territoryName = Westboro, regionID = 1)\n",
      "(territoryID = 2903, territoryName = Providence, regionID = 1)\n",
      "(territoryID = 8837, territoryName = Edison, regionID = 1)\n",
      "(territoryID = 19428, territoryName = Philadelphia, regionID = 3)\n",
      "(territoryID = 40222, territoryName = Louisville, regionID = 1)\n",
      "(territoryID = 45839, territoryName = Findlay, regionID = 3)\n",
      "(territoryID = 55113, territoryName = Roseville, regionID = 3)\n",
      "(territoryID = 1833, territoryName = Georgetow, regionID = 1)\n",
      "(territoryID = 3049, territoryName = Hollis, regionID = 3)\n",
      "(territoryID = 10019, territoryName = New York, regionID = 1)\n",
      "(territoryID = 19713, territoryName = Neward, regionID = 1)\n",
      "(territoryID = 44122, territoryName = Beachwood, regionID = 3)\n",
      "(territoryID = 48075, territoryName = Southfield, regionID = 3)\n",
      "(territoryID = 55439, territoryName = Minneapolis, regionID = 3)\n",
      "(territoryID = 2116, territoryName = Boston, regionID = 1)\n",
      "(territoryID = 3801, territoryName = Portsmouth, regionID = 3)\n",
      "(territoryID = 10038, territoryName = New York, regionID = 1)\n",
      "(territoryID = 20852, territoryName = Rockville, regionID = 1)\n",
      "(territoryID = 48084, territoryName = Troy, regionID = 3)\n",
      "(territoryID = 2139, territoryName = Cambridge, regionID = 1)\n",
      "(territoryID = 6897, territoryName = Wilton, regionID = 1)\n",
      "(territoryID = 11747, territoryName = Mellvile, regionID = 1)\n",
      "(territoryID = 27403, territoryName = Greensboro, regionID = 1)\n",
      "Even\n",
      "(territoryID = 31406, territoryName = Savannah, regionID = 4)\n",
      "(territoryID = 90405, territoryName = Santa Monica, regionID = 2)\n",
      "(territoryID = 95054, territoryName = Santa Clara, regionID = 2)\n",
      "(territoryID = 98104, territoryName = Seattle, regionID = 2)\n",
      "(territoryID = 72716, territoryName = Bentonville, regionID = 4)\n",
      "(territoryID = 32859, territoryName = Orlando, regionID = 4)\n",
      "(territoryID = 80909, territoryName = Colorado Springs, regionID = 2)\n",
      "(territoryID = 94025, territoryName = Menlo Park, regionID = 2)\n",
      "(territoryID = 95060, territoryName = Santa Cruz, regionID = 2)\n",
      "(territoryID = 75234, territoryName = Dallas, regionID = 4)\n",
      "(territoryID = 29202, territoryName = Columbia, regionID = 4)\n",
      "(territoryID = 33607, territoryName = Tampa, regionID = 4)\n",
      "(territoryID = 85014, territoryName = Phoenix, regionID = 2)\n",
      "(territoryID = 94105, territoryName = San Francisco, regionID = 2)\n",
      "(territoryID = 98004, territoryName = Bellevue, regionID = 2)\n",
      "(territoryID = 60179, territoryName = Hoffman Estates, regionID = 2)\n",
      "(territoryID = 78759, territoryName = Austin, regionID = 4)\n",
      "(territoryID = 30346, territoryName = Atlanta, regionID = 4)\n",
      "(territoryID = 85251, territoryName = Scottsdale, regionID = 2)\n",
      "(territoryID = 95008, territoryName = Campbell, regionID = 2)\n",
      "(territoryID = 98052, territoryName = Redmond, regionID = 2)\n",
      "(territoryID = 60601, territoryName = Chicago, regionID = 2)\n",
      "(territoryID = 80202, territoryName = Denver, regionID = 2)\n"
     ]
    }
   ],
   "source": [
    "! echo \"Odd\" && cat /tmp/outputs_odd* && echo \"Even\" && cat /tmp/outputs_even*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6574254-039b-49ee-b81d-c12aec4d740d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64199d-86e5-4a52-b8d5-e56b34685278",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184af67-b4bc-4dfc-b8c8-a85735022725",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. To use Group, Join, Sort you need to reshape the data into a KV pair first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c69d69-d541-4d25-aae0-6a165c2f6531",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040dd12-e0d2-4eeb-ba6f-ec490542e822",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ___WithKeys___ will reshape your data first, then GroupByKey will cluster the elements as a list under each unique key. The data must be in a KV tuple pair first. Also not how to read a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6f82f-8a76-49b2-9fc6-d5c8c9651d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "import json\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = int(element['territoryid']), element['territoryname'], int(element['regionid'])\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/JSON/territories/territories.json'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText(territoriesfilename)\n",
    "                    | 'From json' >> beam.Map(json.loads)\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x.regionid)\n",
    "#                    | 'Group Territories' >> beam.GroupByKey() \n",
    "                    | 'Print Territories' >> beam.Map(print)\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986d67f-7a28-4323-8943-cbab751ee1a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Combine is equivalent to a SQL ___GROUP BY___ query\n",
    "### ___SELECT key, sum(value) as total FROM source GROUP BY key___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c53775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', 10), ('a', 20), ('b', 30), ('b', 40), ('c', 50), ('a', 60)])\n",
    "          | 'Combine' >> beam.CombinePerKey(sum)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542b61e-9a43-4c33-b05b-b18ea5d0ac39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Custom Combine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e60d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class CustomCombine(beam.CombineFn):\n",
    "    \"\"\"\n",
    "    This custom combiner will calculate the max of the first element, sum of the second element and a count of total elements\n",
    "    The final step will also return the average of the second element.\n",
    "    \"\"\"\n",
    "    def create_accumulator(self):\n",
    "        # method defining how to create an empty accumulator\n",
    "        return dict()\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        # get the input and split it up for easier manipulation\n",
    "        k, v = input\n",
    "        # get the values from the accumulator for the input key or initialize it if it's the first time we see this key\n",
    "        m, s, c = accumulator.get(k, (0, 0, 0))\n",
    "\n",
    "        # take the max for the first element of the tuple and sum the second element and count for the third\n",
    "        accumulator[k] = (v[0] if v[0] > m else m, s + v[1], c + 1)\n",
    "        return accumulator\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        # merge the accumulators from the various workers once they have finished accumulating locally\n",
    "        merged = dict()\n",
    "        for accum in accumulators:\n",
    "          for k, v in accum.items():\n",
    "            m, s, c = merged.get(k, (0, 0, 0))\n",
    "            merged[k] = (v[0] if v[0] > m else m, s + v[1], c + v[2])\n",
    "        return merged\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        # called when all the works accumulators have been merge to render the final output\n",
    "        # return the max, the sum, the count and the average for the key\n",
    "        return {k : (v[0], v[1], v[2], v[1]/v[2]) for k, v in accumulator.items()}\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', (1, 10)), ('a', (2, 20)), \n",
    "                                     ('b', (3, 30)), ('c', (5, 50)), \n",
    "                                     ('b', (4, 40)), ('a', (6, 60))])\n",
    "          | 'Combine' >> beam.CombineGlobally(CustomCombine())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97109e52-fbf6-42eb-8266-fec461471367",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a nested repeating output\n",
    "### First create a dataset. Here is Python code for the equivalent bq command of bq mk dataflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as doing bq mk dataflow\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set dataset_id to the ID of the dataset to create.\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-b1b7cded1c4b'\n",
    "dataset_id = f\"{PROJECT_ID}.dataflow\" #.format(client.project)\n",
    "\n",
    "# TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# # Construct a full Dataset object to send to the API.\n",
    "# dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "\n",
    "try:\n",
    "    client.get_dataset(dataset_id)  # Make an API request.\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except:\n",
    "    print(\"Dataset {} is not found\".format(dataset_id))\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "    \n",
    "    \n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"regionid\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"regionname\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"territories\", \"RECORD\", mode=\"REPEATED\", \n",
    "            fields=[\n",
    "                    bigquery.SchemaField(\"territoryid\", \"STRING\", mode=\"REQUIRED\"),\n",
    "                    bigquery.SchemaField(\"territoryname\", \"STRING\", mode=\"REQUIRED\")\n",
    "                   ]\n",
    "                        )\n",
    "]\n",
    "\n",
    "# create table dataflow.region_territory\n",
    "# (regionid NUMERIC\n",
    "# ,regionname STRING\n",
    "# ,territories ARRAY<STRUCT<territoryid NUMERIC, territoryname STRING>>)\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.dataflow.region_territory\"\n",
    "\n",
    "try:\n",
    "    table = client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    print(table)\n",
    "except:\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    table = client.create_table(table)  # Make an API request.\n",
    "    print(\"Table {} created.\".format(table_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586c889-1dcb-43be-a82a-cbee16e6a6b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sometimes need to manually create a schema for a nested repeating because it cannot use a simple string. In this case we don't really need it but it's included here as a reference in case we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f17f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.io.gcp.internal.clients import bigquery as bq\n",
    "region_territory_schema = bq.TableSchema()\n",
    "regionid = bq.TableFieldSchema(name = 'regionid', type = 'string', mode = 'required')\n",
    "region_territory_schema.fields.append(regionid)\n",
    "regionname = bq.TableFieldSchema(name = 'regionname', type = 'string', mode='required')\n",
    "region_territory_schema.fields.append(regionname)\n",
    "\n",
    "# A nested field\n",
    "territories = bq.TableFieldSchema(name = 'territories', type = 'record', mode = 'nullable')\n",
    "territoryid = bq.TableFieldSchema(name = 'territoryid', type = 'string', mode = 'required')\n",
    "territories.fields.append(territoryid)\n",
    "territoryname = bq.TableFieldSchema(name = 'territoryname', type = 'string', mode = 'required')\n",
    "territories.fields.append(territoryname)\n",
    "\n",
    "region_territory_schema.fields.append(territories)\n",
    "\n",
    "print(region_territory_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f289745-bf0e-473f-81f0-943bc1232ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The code here is tricky: \n",
    "### First parse the two tables into tuples, (regionid, regionname) & (regionid, {'territoryid':territoryid, 'territoryname':territoryname})\n",
    "### CoGroupByKey yields a shape like (regionid, {'regions':['regionname'], 'territories':[{}]) so we need to reshape it to dicts to write it to BQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce154df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(regionid), {'territoryid': int(territoryid), 'territoryname':territoryname})\n",
    "\n",
    "class SortTerritories(beam.DoFn):\n",
    "    #{'regionid': 1, 'regionname': 'Eastern', 'territories': [{'territoryid': 1730, 'territoryname': 'Bedford'}, \n",
    "    def process(self, element):\n",
    "        territories = element['territories']\n",
    "        element['territories'] = sorted(territories, key = lambda x : x['territoryid'])\n",
    "        yield element\n",
    "\n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "\n",
    "#PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseTuple())\n",
    "#                | 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#                    | 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "    nested = ( \n",
    "        {'regions':regions, 'territories':territories} \n",
    "              | 'Nest territories into regions' >> beam.CoGroupByKey()\n",
    "              | 'Reshape to dict' >> beam.Map(lambda x : {'regionid': x[0], 'regionname': x[1]['regions'][0], \n",
    "                                                        'territories': x[1]['territories']})\n",
    "              | 'Sort by territoryid' >> beam.ParDo(SortTerritories())\n",
    "#              | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    nested | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "                                                                             , project = PROJECT_ID\n",
    "                                                                             , method = 'STREAMING_INSERTS'\n",
    "                                                                             )\n",
    "#    nested | 'Print' >> beam.Map(print)\n",
    "             \n",
    "#help(beam.io.WriteToBigQuery)    \n",
    "#(1, {'regions': ['Eastern'], 'territories': [{'territoryid': 1730, 'territoryname': 'Bedford'}, {'territoryid': 1581, 'territoryname': 'Westboro'}, {'territoryid': 1833, 'territoryname': 'Georgetow'}, {'territoryid': 2116, 'territoryname': 'Bosto\n",
    "#{'regionid': 1, 'regionname':'Eastern', 'territories' : [{'territoryid':1, 'territoryname':'name1'}, {}, {}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae0097-3880-4cbc-9010-078fb1aa10df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query the table to show it was populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411903b-871d-487c-8324-089f330cb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.dataflow.region_territory\"\n",
    "\n",
    "query_job = client.query(f\"\"\"SELECT * FROM {table_id}\"\"\")\n",
    "\n",
    "results = query_job.result()  # Waits for job to complete.\n",
    "display(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bc155-d9d3-419e-8bd8-6b5a41718bdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper functions to make a generic transform to nest children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class NestJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key, sort = lambda x : x):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "        self.sort = sort\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def reshapeToKV(item, key):\n",
    "            # pipeline object should be a dictionary\n",
    "            item1 = item.copy()\n",
    "            del item1[key]\n",
    "            return (item[key], item1)\n",
    "\n",
    "        def reshapeCoGroupToDict(item):\n",
    "            ret = {self.parent_key : item[0]}\n",
    "            ret.update(item[1][self.parent_pipeline_name][0])\n",
    "            ret[self.child_pipeline_name] = item[1][self.child_pipeline_name]\n",
    "            return ret\n",
    "\n",
    "        return (\n",
    "                {\n",
    "                self.parent_pipeline_name : pcols[self.parent_pipeline_name] | f'Convert {self.parent_pipeline_name} to KV' \n",
    "                    >> beam.Map(reshapeToKV, self.parent_key)\n",
    "                ,self.child_pipeline_name : pcols[self.child_pipeline_name] | f'Convert {self.child_pipeline_name} to KV'\n",
    "                    >> beam.Map(reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_pipeline_name} into {self.parent_pipeline_name}'\n",
    "                    >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary'\n",
    "                    >> beam.Map(reshapeCoGroupToDict)\n",
    "                  | f'Sort the nested data' >> beam.Map(self.sort)\n",
    "            \n",
    "        )\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "      \n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "\n",
    "def sort_territories(element):\n",
    "    territories = element['territories']\n",
    "    element['territories'] = list(sorted(territories, key = lambda x : x['territoryid']))\n",
    "    return element\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | NestJoin('regions', 'regionid', 'territories', 'regionid', sort = sort_territories)\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n",
    "#     nestjoin | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "#                                                                              , project = PROJECT_ID\n",
    "#                                                                              , method = 'STREAMING_INSERTS'\n",
    "#                                                                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bdeb1-854c-48df-b8e8-d3f0c81e3b46",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simulate an Outer Join with CoGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108d942-1b57-461e-83e7-70574e318f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class LeftJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def reshapeToKV(item, key):\n",
    "            # pipeline object should be a dictionary\n",
    "            item1 = item.copy()\n",
    "            del item1[key]\n",
    "            return (item[key], item1)\n",
    "\n",
    "        def reshapeCoGroupToFlatDict(item):\n",
    "            parent = {self.parent_key : item[0]}\n",
    "            parent.update(item[1][self.parent_pipeline_name][0])\n",
    "            ret = []\n",
    "            for row1 in item[1][self.child_pipeline_name]:\n",
    "                row = parent.copy()\n",
    "                row.update(row1)\n",
    "                ret.append(row)\n",
    "            return ret\n",
    "\n",
    "        return (\n",
    "                {\n",
    "                self.parent_pipeline_name : pcols[self.parent_pipeline_name] | f'Convert {self.parent_pipeline_name} to KV' \n",
    "                    >> beam.Map(reshapeToKV, self.parent_key)\n",
    "                ,self.child_pipeline_name : pcols[self.child_pipeline_name] | f'Convert {self.child_pipeline_name} to KV'\n",
    "                    >> beam.Map(reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_pipeline_name} into {self.parent_pipeline_name}'\n",
    "                    >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary'\n",
    "                    >> beam.Map(reshapeCoGroupToFlatDict)\n",
    "        )\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', 'regionid', 'territories', 'regionid')\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313cf2f-c440-452a-bf57-129bf2926f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4530d-df0e-4844-aef0-632b63109107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f12065-b9cd-4a07-aa07-7d20633912ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c2dbd91-e4b8-4bd1-8c75-89064613bbd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030bb71-8ee0-48e6-aa39-284b2c65c042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62fc2a94-7ddf-4a23-8581-e58bac8e7240",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548cd66-511d-460d-ab91-42090216ba1e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaad46b-2a1d-40df-a573-02e5716279c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. BeamSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e256-d80c-4465-a6cf-8930c1825f2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986c8bd-d52f-40f8-9638-ef5ccfeb59d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SQL Transform uses PCOLLECTION as the name of a single source passed into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391af3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'territoryid = {self.territoryid} territoryname = {self.territoryname} regionid = {self.regionid}'\n",
    "coders.registry.register_coder(Territory, coders.RowCoder)\n",
    "        \n",
    "@beam.typehints.with_output_types(Territory)\n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname.title(), int(regionid))\n",
    "    \n",
    "class RegionCount(typing.NamedTuple):\n",
    "    regionid: int\n",
    "    count: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'regionid = {self.regionid} count = {self.count}'\n",
    "coders.registry.register_coder(RegionCount, coders.RowCoder)\n",
    "        \n",
    "        \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "#                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass()).with_output_types(Territory) # if we didn't have with_output_types decorator\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `count` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "#                    | 'Map Territories for Print' >> beam.Map(lambda x : f'regionid = {x.regionid}  count = {x.count}')\n",
    "#                    | 'Convert to RegionCount Class' >> beam.Map(lambda x : RegionCount(x.regionid, x.count))\n",
    "                    | 'Print SQL' >> beam.Map(print)\n",
    "                    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c3a7c-0584-4268-a2f5-fbc04534eacc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### For a SQL query that has more than one source, bundle the sources together in a dictionary, they keys become the table names inside the SQL string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc93081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'Create Parent' >> beam.Create([(1, 'Vowel'), (2, 'Consonant'), (4, 'Unknown')])\n",
    "              | 'Map Parent' >> beam.Map(lambda x : beam.Row(parent_id = x[0], parent_name = x[1]))\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'Create Child' >> beam.Create([('Alpha', 1), ('Beta', 2), ('Gamma', 2), ('Delta', 2), ('Epsilon', 1), ('Pi', 3)])\n",
    "              | 'Map Child' >> beam.Map(lambda x : beam.Row(child_name = x[0], parent_id = x[1]))\n",
    "    )\n",
    "    \n",
    "    result = ( {'parent': parent, 'child' : child} \n",
    "         | SqlTransform(\"\"\"\n",
    "             SELECT p.parent_id, p.parent_name, c.child_name \n",
    "             FROM parent as p \n",
    "             INNER JOIN child as c ON p.parent_id = c.parent_id\n",
    "             \"\"\")\n",
    "        | 'Map Join' >> beam.Map(lambda x : f'{x.parent_id}, {x.parent_name}, {x.child_name}')\n",
    "        | 'Print Join' >> beam.Map(print)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe17701-cec9-4a53-b8ea-66f8bf5ad2ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde66d76-9970-40c9-b46a-f5174636606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java \n",
    "\n",
    "\n",
    "\n",
    "  // Define the schema for the records.\n",
    "    Schema appSchema =\n",
    "        Schema\n",
    "          .builder()\n",
    "          .addInt32Field(\"appId\")\n",
    "          .addStringField(\"description\")\n",
    "          .addDateTimeField(\"rowtime\")\n",
    "          .build();\n",
    "\n",
    "    // Create a concrete row with that type.\n",
    "    Row row =\n",
    "        Row\n",
    "          .withSchema(appSchema)\n",
    "          .addValues(1, \"Some cool app\", new Date())\n",
    "          .build();\n",
    "\n",
    "    // Create a source PCollection containing only that row\n",
    "    PCollection<Row> testApps =\n",
    "        PBegin\n",
    "          .in(p)\n",
    "          .apply(Create\n",
    "                    .of(row)\n",
    "                    .withCoder(RowCoder.of(appSchema)));\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "482dd828-86c8-4373-991d-ba1276cd78de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rm src/main/java/samples/quickstart/*.java\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm build/libs/*.jar\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm -rf /tmp/outputs*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m\n",
      "> Task :compileJava FAILED\n",
      "/home/jupyter/dataflowclass1/src/main/java/samples/quickstart/ReadTerritories.java:54: error: incompatible types: inference variable OutputT has incompatible bounds\n",
      "            .apply(\"To Rows\", Convert.toRows())\n",
      "                  ^\n",
      "    equality constraints: PCollection<Row>\n",
      "    upper bounds: PCollection<Territory>,POutput\n",
      "  where OutputT,T are type-variables:\n",
      "    OutputT extends POutput declared in method <OutputT>apply(String,PTransform<? super PCollection<T>,OutputT>)\n",
      "    T extends Object declared in class PCollection\n",
      "1 error\n",
      "\n",
      "FAILURE: Build failed with an exception.\n",
      "\n",
      "* What went wrong:\n",
      "Execution failed for task ':compileJava'.\n",
      "> Compilation failed; see the compiler error output for details.\n",
      "\n",
      "* Try:\n",
      "Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n",
      "\n",
      "* Get more help at https://help.gradle.org\n",
      "\n",
      "BUILD FAILED in 0s\n",
      "1 actionable task: 1 executed\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 0\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m\n",
      "> Task :compileJava FAILED\n",
      "/home/jupyter/dataflowclass1/src/main/java/samples/quickstart/ReadTerritories.java:54: error: incompatible types: inference variable OutputT has incompatible bounds\n",
      "            .apply(\"To Rows\", Convert.toRows())\n",
      "                  ^\n",
      "    equality constraints: PCollection<Row>\n",
      "    upper bounds: PCollection<Territory>,POutput\n",
      "  where OutputT,T are type-variables:\n",
      "    OutputT extends POutput declared in method <OutputT>apply(String,PTransform<? super PCollection<T>,OutputT>)\n",
      "    T extends Object declared in class PCollection\n",
      "1 error\n",
      "\n",
      "FAILURE: Build failed with an exception.\n",
      "\n",
      "* What went wrong:\n",
      "Execution failed for task ':compileJava'.\n",
      "> Compilation failed; see the compiler error output for details.\n",
      "\n",
      "* Try:\n",
      "Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n",
      "\n",
      "* Get more help at https://help.gradle.org\n",
      "\n",
      "BUILD FAILED in 0s\n",
      "1 actionable task: 1 executed\n",
      "\u001b[m\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java verbose nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.schemas.Schema.FieldType;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.JavaBeanSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import org.apache.beam.sdk.schemas.JavaFieldSchema;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        // Define the schema for the records.\n",
    "        Schema territorySchema = Schema\n",
    "          .builder()\n",
    "          .addInt64Field(\"territoryID\")\n",
    "          .addStringField(\"territoryName\")\n",
    "          .addInt64Field(\"regionID\")\n",
    "          .build();\n",
    "\n",
    "        // Define the schema to hold the results.\n",
    "        Schema resultSchema = Schema.of(\n",
    "            Schema.Field.of(\"regionID\", Schema.FieldType.INT64), \n",
    "            Schema.Field.of(\"cnt\", Schema.FieldType.INT64));\n",
    "        \n",
    "        \n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"To Rows\", Convert.toRows())\n",
    "        ;                   \n",
    "        \n",
    "        \n",
    "          PCollection<Row> territories3 = territories.apply(\n",
    "             SqlTransform.query(\"SELECT regionID, COUNT(*) as cnt from PCOLLECTION GROUP BY regionID\")).setRowSchema(resultSchema);\n",
    "        \n",
    "          territories3.apply(\n",
    "              \"Print\", MapElements.via(new SimpleFunction<Row, Row>() {\n",
    "                  @Override\n",
    "                  public Row apply(Row input) {\n",
    "                      System.out.println(\"SQL Result: \" + input.getValues());\n",
    "                      return input;\n",
    "                  }\n",
    "              }\n",
    "          )).setRowSchema(resultSchema);\n",
    "        \n",
    "//        territories3.apply(TextIO.<Row>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "/*    \n",
    "    \n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Region {\n",
    "        Long regionID;\n",
    "        Long cnt regionName;\n",
    "        \n",
    "        Region() {}\n",
    "        \n",
    "        Region(long regionID, long cnt) {\n",
    "            this.regionID = regionID;\n",
    "            this.cnt = cnt;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionID = %d, cnt = %d)\", regionID, cnt);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Region, String> {\n",
    "        @Override\n",
    "        public String apply(Region input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "private class Transform extends PTransform<pcollectionlist<row>, PCollection<row>> {\n",
    " \n",
    "    @Override\n",
    "    public PCollection<row> expand(PCollectionList<row> pinput) {\n",
    "      checkArgument(\n",
    "          pinput.size() == 1,\n",
    "          \"Wrong number of inputs for %s: %s\",\n",
    "          BeamUncollectRel.class.getSimpleName(),\n",
    "          pinput);\n",
    "      PCollection<row> upstream = pinput.get(0);\n",
    " \n",
    "      // Each row of the input contains a single array of things to be emitted; Calcite knows\n",
    "      // what the row looks like\n",
    "      Schema outputSchema = CalciteUtils.toSchema(getRowType());\n",
    " \n",
    "      PCollection<row> uncollected =\n",
    "          upstream.apply(ParDo.of(new UncollectDoFn(outputSchema))).setRowSchema(outputSchema);\n",
    " \n",
    "      return uncollected;\n",
    "    }\n",
    "  }    \n",
    "    static class ParseRegions extends DoFn<Row, Region> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            \n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "\n",
    "@DefaultSchema(AutoValueSchema.class)\n",
    "@AutoValue\n",
    "public abstract class MyPersonClass {\n",
    "  public abstract String getName();\n",
    "  public abstract Integer getAge();\n",
    "  public abstract Float getHeight();\n",
    "\n",
    "  @SchemaCreate\n",
    "  public static MyPersonClass create(String name, Integer age, Float height) {\n",
    "    return new AutoValue_MyPersonClass(name, age, height);\n",
    "  }\n",
    "}\n",
    "    \n",
    "*/\n",
    "    \n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5490fb-33e0-4949-9f1a-45c5d4254aad",
   "metadata": {},
   "source": [
    "## BeamSQL Java working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb24f9f2-4b5f-47a8-9718-8b553c2814fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rm src/main/java/samples/quickstart/*.java\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm build/libs/*.jar\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ">> rm -rf /tmp/outputs*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes\n",
      "> Task :jar\n",
      "> Task :startScripts UP-TO-DATE\n",
      "> Task :distTar\n",
      "> Task :distZip\n",
      "> Task :shadowJar\n",
      "> Task :startShadowScripts\n",
      "> Task :shadowDistTar\n",
      "> Task :shadowDistZip\n",
      "> Task :assemble\n",
      "> Task :compileTestJava NO-SOURCE\n",
      "> Task :processTestResources NO-SOURCE\n",
      "> Task :testClasses UP-TO-DATE\n",
      "> Task :test NO-SOURCE\n",
      "> Task :check UP-TO-DATE\n",
      "> Task :build\n",
      "\n",
      "BUILD SUCCESSFUL in 26s\n",
      "9 actionable tasks: 8 executed, 1 up-to-date\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 68M\n",
      "-rw-r--r-- 1 root root  68M Dec 28 03:05 ReadTerritories.jar\n",
      "-rw-r--r-- 1 root root 7.0K Dec 28 03:05 dataflowclass1.jar\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava UP-TO-DATE\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes UP-TO-DATE\n",
      "> Task :shadowJar UP-TO-DATE\n",
      "> Task :startShadowScripts UP-TO-DATE\n",
      "> Task :installShadowDist\n",
      "\n",
      "> Task :runShadow\n",
      "Dec 28, 2021 3:06:06 AM org.apache.beam.sdk.extensions.sql.impl.CalciteQueryPlanner convertToBeamRel\n",
      "INFO: SQL:\n",
      "SELECT `PCOLLECTION`.`regionID`, COUNT(*) AS `cnt`\n",
      "FROM `beam`.`PCOLLECTION` AS `PCOLLECTION`\n",
      "GROUP BY `PCOLLECTION`.`regionID`\n",
      "Dec 28, 2021 3:06:06 AM org.apache.beam.sdk.extensions.sql.impl.CalciteQueryPlanner convertToBeamRel\n",
      "INFO: SQLPlan>\n",
      "LogicalAggregate(group=[{0}], cnt=[COUNT()])\n",
      "  LogicalProject(regionID=[$2])\n",
      "    BeamIOSourceRel(table=[[beam, PCOLLECTION]])\n",
      "\n",
      "Dec 28, 2021 3:06:06 AM org.apache.beam.sdk.extensions.sql.impl.CalciteQueryPlanner convertToBeamRel\n",
      "INFO: BEAMPlan>\n",
      "BeamAggregationRel(group=[{2}], cnt=[COUNT()])\n",
      "  BeamIOSourceRel(table=[[beam, PCOLLECTION]])\n",
      "\n",
      "Dec 28, 2021 3:06:08 AM org.apache.beam.sdk.io.FileBasedSource getEstimatedSizeBytes\n",
      "INFO: Filepattern datasets/northwind/CSV/territories/territories.csv matched 1 files with total size 933\n",
      "Dec 28, 2021 3:06:08 AM org.apache.beam.sdk.io.FileBasedSource split\n",
      "INFO: Splitting filepattern datasets/northwind/CSV/territories/territories.csv into bundles of size 233 took 2 ms and produced 1 files and 4 bundles\n",
      "SQL Result: [4, 8]\n",
      "SQL Result: [2, 15]\n",
      "SQL Result: [1, 19]\n",
      "SQL Result: [3, 11]\n",
      "\n",
      "BUILD SUCCESSFUL in 8s\n",
      "5 actionable tasks: 2 executed, 3 up-to-date\n",
      "\u001b[m\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java verbose nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.schemas.Schema.FieldType;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import com.google.auto.value.AutoValue;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "        // Define the schema for the records.\n",
    "        Schema territorySchema = Schema\n",
    "          .builder()\n",
    "          .addInt64Field(\"territoryID\")\n",
    "          .addStringField(\"territoryName\")\n",
    "          .addInt64Field(\"regionID\")\n",
    "          .build();\n",
    "        // Define the schema to hold the results.\n",
    "        \n",
    "        Schema resultSchema = Schema.of(\n",
    "            Schema.Field.of(\"regionID\", Schema.FieldType.INT64), \n",
    "            Schema.Field.of(\"cnt\", Schema.FieldType.INT64));\n",
    "        \n",
    "        // Convert them to Rows with the same schema as defined above via a DoFn.\n",
    "        PCollection<Row> territories2 = territories\n",
    "          .apply(\n",
    "          ParDo.of(new DoFn<Territory, Row>() {\n",
    "            @ProcessElement\n",
    "            public void process(ProcessContext c) {\n",
    "              // Get the current POJO instance\n",
    "              Territory t = c.element();\n",
    "\n",
    "              // Create a Row with the appSchema schema\n",
    "              // and values from the current POJO\n",
    "              Row territoryRow =\n",
    "                    Row\n",
    "                      .withSchema(territorySchema)\n",
    "                      .addValues(\n",
    "                        t.territoryID,\n",
    "                        t.territoryName,\n",
    "                        t.regionID)\n",
    "                      .build();\n",
    "\n",
    "              // Output the Row representing the current POJO\n",
    "              c.output(territoryRow);\n",
    "            }\n",
    "          })).setRowSchema(territorySchema);\n",
    "        \n",
    "          PCollection<Row> territories3 = territories2.apply(Convert.toRows()).apply(\n",
    "             SqlTransform.query(\"SELECT regionID, COUNT(*) as cnt from PCOLLECTION GROUP BY regionID\")).setRowSchema(resultSchema);\n",
    "        \n",
    "          territories3.apply(\n",
    "              \"Print\", MapElements.via(new SimpleFunction<Row, Row>() {\n",
    "                  @Override\n",
    "                  public Row apply(Row input) {\n",
    "                      System.out.println(\"SQL Result: \" + input.getValues());\n",
    "                      return input;\n",
    "                  }\n",
    "              }\n",
    "          )).setRowSchema(resultSchema);\n",
    "//        territories3.apply(TextIO.<Row>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "\n",
    "/*    \n",
    "    @schemultSchema(AutoValueSchema.class)\n",
    "    @AutoValue\n",
    "    public static abstract class Territory {\n",
    "      public abstract Long getTerritoryID();\n",
    "      public abstract String getTerritoryName();\n",
    "      public abstract Long getRegionID();\n",
    "\n",
    "      @SchemaCreate\n",
    "      public static Territory create(Long territoryID, String territoryName, Long regionID) {\n",
    "        return new AutoValue_TerritoryClass(territoryID, territoryName, regionID);\n",
    "      }\n",
    "*/    \n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "/*    \n",
    "    \n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Region {\n",
    "        Long regionID;\n",
    "        Long cnt regionName;\n",
    "        \n",
    "        Region() {}\n",
    "        \n",
    "        Region(long regionID, long cnt) {\n",
    "            this.regionID = regionID;\n",
    "            this.cnt = cnt;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionID = %d, cnt = %d)\", regionID, cnt);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Region, String> {\n",
    "        @Override\n",
    "        public String apply(Region input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "private class Transform extends PTransform<pcollectionlist<row>, PCollection<row>> {\n",
    " \n",
    "    @Override\n",
    "    public PCollection<row> expand(PCollectionList<row> pinput) {\n",
    "      checkArgument(\n",
    "          pinput.size() == 1,\n",
    "          \"Wrong number of inputs for %s: %s\",\n",
    "          BeamUncollectRel.class.getSimpleName(),\n",
    "          pinput);\n",
    "      PCollection<row> upstream = pinput.get(0);\n",
    " \n",
    "      // Each row of the input contains a single array of things to be emitted; Calcite knows\n",
    "      // what the row looks like\n",
    "      Schema outputSchema = CalciteUtils.toSchema(getRowType());\n",
    " \n",
    "      PCollection<row> uncollected =\n",
    "          upstream.apply(ParDo.of(new UncollectDoFn(outputSchema))).setRowSchema(outputSchema);\n",
    " \n",
    "      return uncollected;\n",
    "    }\n",
    "  }    \n",
    "    static class ParseRegions extends DoFn<Row, Region> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            \n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "*/\n",
    "    \n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66766ce8-77db-471d-8938-f6bfada4c0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd2ffb3-9158-4ae6-9c8e-999f9a0ab3b8",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a8d33-1cd4-42c6-8277-da98c7cf15a8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f139a1-db8c-41b5-9881-1a5eb2570f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8. DoFn Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cb066-030c-4846-a446-eeada7ddd4af",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143846d-38ce-438a-bd4d-85e0838f716f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DoFn Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3951161-ad01-4a5b-aee3-9e3b220883d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def setup(self):\n",
    "        self.lookup = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        print('setup')\n",
    "        \n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "        \n",
    "    def process(self, element, uppercase = 0):\n",
    "        #lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = self.lookup.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), uppercase = 1 ) \n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c622d-8a76-41da-afb2-60258b499577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ac07ff-2799-4372-8e75-9c35d849a6d6",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a99c8-0cad-4c18-ad35-579923651e25",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e93fc8-c37b-4f8c-b56e-8e114161ef10",
   "metadata": {},
   "source": [
    "# 9. Side Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfa540-e59d-44b2-a561-84d5142cf63e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8562667-e840-48c5-a9e3-db3432ae2d4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Side inputs are about passing extra parameters to a function where the parameters are calculated in the pipeline itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.transforms.combiners import Sample\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element, uppercase = '0'):\n",
    "        # It's a bit weird here but what is passed in is a single element array of a string\n",
    "        #print('***', uppercase)\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname if uppercase[0] == '0' else territoryname.upper(), int(regionid))\n",
    "\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "    sideinput = (\n",
    "        p | 'Read sideinput.txt' >> ReadFromText('sideinput.txt')\n",
    "          | Sample.FixedSizeGlobally(1)\n",
    "    )\n",
    "    \n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple(), uppercase = [\"0\"]) # This is not a side input but just passing a fixed parameter\n",
    "#          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple(), uppercase = sideinput)  # fails because sideinput is a PCollection not an integer\n",
    "#          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple(), uppercase = beam.pvalue.AsSingleton(sideinput))  # When the parameter is calculated in the pipeline itself, that makes it a side input\n",
    "          | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#    maxregion | 'Print Min' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9668295-b173-48d2-8868-cc72fbaa6927",
   "metadata": {},
   "source": [
    "### Side input that is a lookup list\n",
    "### More realistic example where the entire lookup table is read in the pipeline then distributed to each worker as a side input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "        # {1:'North', 2:'South'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        # Becase the regions PCollection is a different shape, use the following comprehension to make it easier to do a lookup\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable } # {1:'North', 2:'South'}\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9be7e3-8313-44d5-a323-e361227209ab",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923315b5-84f3-43af-9c68-effabfbf7c56",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83539780-8b06-464e-b9a2-f6aaddd17be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.34.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.34.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
