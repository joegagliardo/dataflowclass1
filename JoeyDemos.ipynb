{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c02722",
   "metadata": {},
   "source": [
    "## Simple transformation\n",
    "<details><summary>Click for <b>Spark</b></summary>\n",
    "<p>\n",
    "\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "rdd1 = ( sc.parallelize(['one', 'two', 'three', 'four'])\n",
    "           .map(str.title)\n",
    "       )\n",
    "rdd1.collect()\n",
    "```\n",
    "</p>\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | 'Create' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | 'Uppercase' >> beam.Map(str.title)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "# lines is a PCollection object\n",
    "print('lines = ', lines)\n",
    "\n",
    "# This is basic Python to do the same thing\n",
    "x = ['one', 'two', 'three', 'four']\n",
    "print(list(map(str.title, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335463d5",
   "metadata": {},
   "source": [
    "## The pipe `|` is actually just an operator overload to call the apply method of the pipeline. You would never do this in python, but it helps to understand what is going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        lines = ((p | 'Create' >> beam.Create(['one', 'two', 'three', 'four']))\n",
    "             .apply(beam.Map(str.title), label = 'titlecase') \n",
    "             .apply(beam.Map(print), label='print')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7de63b",
   "metadata": {},
   "source": [
    "## Read from CSV and use Map.\n",
    "<br>\n",
    "<details><summary>Click for <b>Java</b></summary>\n",
    "<p> \n",
    "    \n",
    "```java\n",
    "\n",
    "package com.mypackage;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.PipelineResult;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "\n",
    "\n",
    "public class Simple1 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"regions.csv\";\n",
    "        String regionsOutputFileName = \"output/regions\";\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", MapElements.into(TypeDescriptors.strings()).via((String element) -> element + \"*\"));\n",
    "        regions.apply(\"Write\", TextIO.write().to(regionsOutputFileName));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n",
    "                       \n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "<details><summary>Click for <b>Python</b></summary>\n",
    "<p>\n",
    "\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "def splitregion(line):\n",
    "    #1,North\n",
    "    return tuple(line.split(','))\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "#          | 'Parse' >> beam.Map(lambda x : tuple(x.split(',')))\n",
    "          | 'Parse' >> beam.Map(splitregion)\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]) * 10, x[1].upper()))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run()\n",
    "```\n",
    "</p>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "def splitregion(element):\n",
    "    #1,North --> ('1','North')\n",
    "    return tuple(element.split(','))\n",
    "\n",
    "def splitregion2(element):\n",
    "    #1,North --> (1,'NORTH')\n",
    "    x = element.split(',')\n",
    "    return (int(x[0]), x[1].upper())\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "            | 'Parse' >> beam.Map(splitregion)\n",
    "#          | 'Parse' >> beam.Map(splitregion2)\n",
    "#          | 'Parse' >> beam.Map(lambda x : tuple(x.split(',')))\n",
    "#          | 'Transform' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "#          | 'Write' >> WriteToText(regionsfilename + '.out')\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run() # implicit in Pytho when using with block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c575a89",
   "metadata": {},
   "source": [
    "## Read from CSV and use ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self,flatt):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "#        yield (int(regionid), regionname.upper()) # Include a transformation instead of doing it as a separate step\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "#          | 'Upper' >> beam.Map(lambda x : (x[0], x[1].upper()))\n",
    "          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat regions.out*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e65d02",
   "metadata": {},
   "source": [
    "## Template showing a full program that can read the command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A template to import the default package and parse the arguments\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re, os\n",
    "\n",
    "#from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  projectid = os.environ.get('PROJECT')\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default=f'gs://{projectid}/regions.csv',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      default = f'gs://{projectid}/regions_output',      \n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # python importregion.py --input gs://projectid/regions1.csv --output gs://projectid/regions1_out --runner \n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionParseTuple())\n",
    "    uppercase = records | 'Uppercase' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "    uppercase | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a6df1",
   "metadata": {},
   "source": [
    "## Example of how to create a split ParDo with multiple outputs or splitting the data up to send it down different paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7649fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class OddEvenRegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        if int(regionid) % 2 == 0:\n",
    "            yield pvalue.TaggedOutput('Even', (int(regionid), regionname, 'Even'))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput('Odd', (int(regionid), regionname, 'Odd'))\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(regionsfilename) \n",
    "    # lines would return a tuple of the two tagged outputs\n",
    "    # unpack the two outputs to two separate variables to process differently\n",
    "    evens, odds = lines | 'Parse' >> beam.ParDo(OddEvenRegionParseTuple()).with_outputs(\"Even\", \"Odd\")\n",
    "    \n",
    "    print('Evens')\n",
    "    (evens \n",
    "        | 'Upper' >> beam.Map(lambda x : (x[0], x[1].upper()))\n",
    "        | 'Print Evens' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    print('Odds')\n",
    "    (\n",
    "    odds \n",
    "        | 'Lower' >> beam.Map(lambda x : (x[0], x[1].lower()))\n",
    "        | 'Print Odds' >> beam.Map(print)\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89aa94",
   "metadata": {},
   "source": [
    "## Example of branching or taking the same data and sending it down multiple paths, such as to group it on two different keys with one read from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "    )\n",
    "    \n",
    "    # Just use the same variable for the PCollection twice. As long as it's inside the with block for the pipeline\n",
    "    # it is all one continuous pipeline process\n",
    "    \n",
    "    # Branch 1\n",
    "    (regions \n",
    "         | 'Lowercase regions' >> beam.Map(lambda x : (x[0] * 100, x[1].lower()))\n",
    "         | 'Write' >> WriteToText('regions2.out')\n",
    "    )\n",
    "    # Branch 2\n",
    "    (regions \n",
    "         | 'Uppercase regions' >> beam.Map(lambda x : (x[0] * 10, x[1].upper()))\n",
    "         | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cb6cc",
   "metadata": {},
   "source": [
    "## WithKeys is a convenience function to create the KV tuple that is commonly used to reshape data in preparation for PTransforms like GroupByKey and CoGroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield (int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText(territoriesfilename)\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x[2])\n",
    "#                    | 'With Keys Manually' >> beam.Map(lambda x : (x[2], x))\n",
    "#                    | 'With Keys Manually removing the key from the second' >> beam.Map(lambda x : (x[2], (x[0], x[1])))\n",
    "    )\n",
    "    territories | 'Print KV' >> beam.Map(print)\n",
    "#    territories | beam.util.Keys() | 'Print Keys' >> beam.Map(print)\n",
    "#    territories | beam.util.Values() | 'Print Values' >> beam.Map(print)\n",
    "\n",
    "# (1, 'North')\n",
    "# (1730, 'Bedford', 1) --> (1, (1730, 'Bedford', 1))\n",
    "# (1730, 'Bedford', 1) --> (1, (1730, 'Bedford'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec28a1f",
   "metadata": {},
   "source": [
    "## GroupByKey will cluster the elements as a list under each unique key. The data must be in a KV tuple pair first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df396c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "#        yield (int(territoryid), territoryname, int(regionid))  # traditional tuple of 3 elements would require reshaping\n",
    "        yield (int(regionid), (int(territoryid), territoryname)) # tuple of 2 elements with 2nd being another tuple \n",
    "\n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x[2])\n",
    "                    | 'Group Territories' >> beam.GroupByKey() \n",
    "#                    | 'Print Territories' >> beam.Map(print)\n",
    "                    | 'Write' >> WriteToText('territories_group.out')\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d21882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls territories_group.out*\n",
    "! cat territories_group.out*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44d0c9",
   "metadata": {},
   "source": [
    "## Flatten is the equivalent of UNION ALL in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines1 = p | 'Create 1' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "    lines2 = p | 'Create 2' >> beam.Create(['alpha', 'beta', 'gamma', 'delta'])\n",
    "\n",
    "    merged = ((lines1, lines2) | 'Merge PCollections' >> beam.Flatten())\n",
    "    merged | beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee8848",
   "metadata": {},
   "source": [
    "## Combine\n",
    "### SELECT key, sum(value) as total FROM source GROUP BY key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c53775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', 10), ('a', 20), ('b', 30), ('b', 40), ('c', 50), ('a', 60)])\n",
    "          | 'Combine' >> beam.CombinePerKey(sum)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c162a5a",
   "metadata": {},
   "source": [
    "## Custom Combine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e60d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport apache_beam as beam\n",
    "\n",
    "class CustomCombine(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        # method defining how to create an empty accumulator\n",
    "        return dict()\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        # get the input and split it up for easier manipulation\n",
    "        k, v = input\n",
    "        # get the values from the accumulator for the input key or initialize it if it's the first time we see this key\n",
    "        x, y, z = accumulator.get(k, (0, 0, 0))\n",
    "\n",
    "        # take the max for the first element of the tuple and sum the second element and count for the third\n",
    "        accumulator[k] = (v[0] if v[0] > x else x, y + v[1], z + 1)\n",
    "        return accumulator\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        # merge the accumulators from the various workers once they have finished accumulating locally\n",
    "        merged = dict()\n",
    "        for accum in accumulators:\n",
    "          for k, v in accum.items():\n",
    "            x, y, z = merged.get(k, (0, 0, 0))\n",
    "            merged[k] = (v[0] if v[0] > x else x, y + v[1], z + v[2])\n",
    "        return merged\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        # called when all the works accumulators have been merge to render the final output\n",
    "        # return the max, the sum, the count and the average for the key\n",
    "        return {k : (v[0], v[1], v[2], v[1]/v[2]) for k, v in accumulator.items()}\n",
    "        return accumulator\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', (1, 10)), ('a', (2, 20)), \n",
    "                                     ('b', (3, 30)), ('c', (5, 50)), \n",
    "                                     ('b', (4, 40)), ('a', (6, 60))])\n",
    "          | 'Combine' >> beam.CombineGlobally(CustomCombine())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8c8c9",
   "metadata": {},
   "source": [
    "## Map vs FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c052feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  plants = (\n",
    "      pipeline\n",
    "      | 'Gardening plants' >> beam.Create(['Strawberry,Carrot,Eggplant','Tomato,Potato'])\n",
    "#      | 'Split words' >> beam.Map(lambda x : x.split(','))\n",
    "      | 'Split words' >> beam.FlatMap(lambda x : x.split(','))\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff1058",
   "metadata": {},
   "source": [
    "## Parsing the line as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8219a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[1].startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseTuple())\n",
    "#           | 'Filter 1' >> beam.Filter(lambda x : x[2] % 2 == 0)\n",
    "#           | 'Filter 2' >> beam.Filter(lambda x : x[1].startswith('S'))\n",
    "          | 'Filter 2' >> beam.ParDo(StartsWithSFilter())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331967c8",
   "metadata": {},
   "source": [
    "## Parse as dict instead so you can use keys instead of positions to refer to elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b177879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid' : int(territoryid), 'territoryname' : territoryname, 'regionid'  : int(regionid)}\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[1].startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseDict())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x['regionid'] % 2 == 0)\n",
    "          | 'Filter 2' >> beam.Filter(lambda x : x['territoryname'].startswith('S'))\n",
    "#          | 'Print' >> beam.Map(print)\n",
    "          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a3b5b",
   "metadata": {},
   "source": [
    "## Parse as a model class based on typing.NamedTuple so you can use properties instead of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0dd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element.territoryname.startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x.regionid % 2 == 0)\n",
    "          | 'Filter 2' >> beam.Filter(lambda x : x.territoryname.startswith('S'))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78a454",
   "metadata": {},
   "source": [
    "## Side Inputs with a single value\n",
    "### Side inputs are about passing extra parameters to a function where the parameters are calculated in the pipeline itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, uppercase = 0):\n",
    "        lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = lookuptable.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    # This is contrived, but let's just calculate a value that we can pass as a single side input to LookupRegion\n",
    "    minregion = (\n",
    "        territories\n",
    "          | 'Extract RegionID' >> beam.Map(lambda x : x[2])\n",
    "          | 'MaxTerritories' >> beam.CombineGlobally(lambda elements: min(elements or [None]))\n",
    "    )\n",
    "\n",
    "    lookup = (\n",
    "        territories\n",
    "#          | beam.ParDo(LookupRegion(), uppercase = 1 ) # This is not a side input but just passing a parameter\n",
    "#          | beam.ParDo(LookupRegion(), uppercase = minregion ) # fails because minregion is a PCollection not an integer\n",
    "          | beam.ParDo(LookupRegion(), uppercase = beam.pvalue.AsSingleton(minregion) ) # When the parameter is calculated in the pipeline itself, that makes it a side input\n",
    "          | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#    maxregion | 'Print Min' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebce971",
   "metadata": {},
   "source": [
    "## Side input that is a lookup list\n",
    "### More realistic example where the entire lookup table is read in the pipeline then distributed to each worker as a side input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "        # {1:'North', 2:'South'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        # Becase the regions PCollection is a different shape, use the following comprehension to make it easier to do a lookup\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable } # {1:'North', 2:'South'}\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5b1f8",
   "metadata": {},
   "source": [
    "## Create a nested repeating output\n",
    "### First create a dataset. Here is python code for the equivalent bq command of bq mk dataflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as doing bq mk dataflow\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set dataset_id to the ID of the dataset to create.\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "dataset_id = f\"{PROJECT_ID}.dataflow\" #.format(client.project)\n",
    "\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "# TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# Send the dataset to the API for creation, with an explicit timeout.\n",
    "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# exists within the project.\n",
    "dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd4405",
   "metadata": {},
   "source": [
    "## Run the following in a bq query window to create the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b84fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "create table dataflow.region_territory\n",
    "(regionid NUMERIC\n",
    ",regionname STRING\n",
    ",territories ARRAY<STRUCT<territoryid NUMERIC, territoryname STRING>>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926085f8",
   "metadata": {},
   "source": [
    "## Sometimes need to manually create a schema for a nested repeating because it cannot use a simple string. In this case we don't really need it but it's included here as a reference in case we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f17f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.io.gcp.internal.clients import bigquery as bq\n",
    "region_territory_schema = bq.TableSchema()\n",
    "regionid = bq.TableFieldSchema(name = 'regionid', type = 'string', mode = 'required')\n",
    "region_territory_schema.fields.append(regionid)\n",
    "regionname = bq.TableFieldSchema(name = 'regionname', type = 'string', mode='required')\n",
    "region_territory_schema.fields.append(regionname)\n",
    "\n",
    "# A nested field\n",
    "territories = bq.TableFieldSchema(name = 'territories', type = 'record', mode = 'nullable')\n",
    "territoryid = bq.TableFieldSchema(name = 'territoryid', type = 'string', mode = 'required')\n",
    "territories.fields.append(territoryid)\n",
    "territoryname = bq.TableFieldSchema(name = 'territoryname', type = 'string', mode = 'required')\n",
    "territories.fields.append(territoryname)\n",
    "\n",
    "region_territory_schema.fields.append(territories)\n",
    "\n",
    "print(region_territory_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51224b95",
   "metadata": {},
   "source": [
    "## The code here is tricky: \n",
    "### First parse the two tables into tuples, (regionid, regionname) & (regionid, {'territoryid':territoryid, 'territoryname':territoryname})\n",
    "### CoGroupByKey yields a shape like (regionid, {'regions':['regionname'], 'territories':[{}]) so we need to reshape it to dicts to write it to BQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce154df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(regionid), {'territoryid': int(territoryid), 'territoryname':territoryname})\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseTuple())\n",
    "#                | 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#                    | 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "    nested = ( \n",
    "        {'regions':regions, 'territories':territories} \n",
    "              | 'Nest territories into regions' >> beam.CoGroupByKey()\n",
    "              | 'Reshape to dict' >> beam.Map(lambda x : {'regionid': x[0], 'regionname': x[1]['regions'][0], \n",
    "                                                        'territories': x[1]['territories']})\n",
    "#              | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    nested | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "                                                                             , project = PROJECT_ID\n",
    "                                                                             , method = 'STREAMING_INSERTS'\n",
    "                                                                             )\n",
    "#    nested | 'Print' >> beam.Map(print)\n",
    "             \n",
    "#help(beam.io.WriteToBigQuery)    \n",
    "#(1, {'regions': ['Eastern'], 'territories': [{'territoryid': 1730, 'territoryname': 'Bedford'}, {'territoryid': 1581, 'territoryname': 'Westboro'}, {'territoryid': 1833, 'territoryname': 'Georgetow'}, {'territoryid': 2116, 'territoryname': 'Bosto\n",
    "#{'regionid': 1, 'regionname':'Eastern', 'territories' : [{'territoryid':1, 'territoryname':'name1'}, {}, {}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1e8dc",
   "metadata": {},
   "source": [
    "## Helper functions to make a generic transform to nest children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class NestJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def reshapeToKV(item, key):\n",
    "            # pipeline object should be a dictionary\n",
    "            item1 = item.copy()\n",
    "            del item1[key]\n",
    "            return (item[key], item1)\n",
    "\n",
    "        def reshapeCoGroupToDict(item):\n",
    "            ret = {self.parent_key : item[0]}\n",
    "            ret.update(item[1][self.parent_pipeline_name][0])\n",
    "            ret[self.child_pipeline_name] = item[1][self.child_pipeline_name]\n",
    "            return ret\n",
    "\n",
    "        return (\n",
    "                {\n",
    "                self.parent_pipeline_name : pcols[self.parent_pipeline_name] | f'Convert {self.parent_pipeline_name} to KV' \n",
    "                    >> beam.Map(reshapeToKV, self.parent_key)\n",
    "                ,self.child_pipeline_name : pcols[self.child_pipeline_name] | f'Convert {self.child_pipeline_name} to KV'\n",
    "                    >> beam.Map(reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_pipeline_name} into {self.parent_pipeline_name}'\n",
    "                    >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary'\n",
    "                    >> beam.Map(reshapeCoGroupToDict)\n",
    "        )\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | NestJoin('regions', 'regionid', 'territories', 'regionid')\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n",
    "#     nestjoin | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "#                                                                              , project = PROJECT_ID\n",
    "#                                                                              , method = 'STREAMING_INSERTS'\n",
    "#                                                                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46841b5",
   "metadata": {},
   "source": [
    "## Simulate an Outer Join with CoGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class LeftJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def reshapeToKV(item, key):\n",
    "            # pipeline object should be a dictionary\n",
    "            item1 = item.copy()\n",
    "            del item1[key]\n",
    "            return (item[key], item1)\n",
    "\n",
    "        def reshapeCoGroupToFlatDict(item):\n",
    "            parent = {self.parent_key : item[0]}\n",
    "            parent.update(item[1][self.parent_pipeline_name][0])\n",
    "            ret = []\n",
    "            for row1 in item[1][self.child_pipeline_name]:\n",
    "                row = parent.copy()\n",
    "                row.update(row1)\n",
    "                ret.append(row)\n",
    "            return ret\n",
    "\n",
    "        return (\n",
    "                {\n",
    "                self.parent_pipeline_name : pcols[self.parent_pipeline_name] | f'Convert {self.parent_pipeline_name} to KV' \n",
    "                    >> beam.Map(reshapeToKV, self.parent_key)\n",
    "                ,self.child_pipeline_name : pcols[self.child_pipeline_name] | f'Convert {self.child_pipeline_name} to KV'\n",
    "                    >> beam.Map(reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_pipeline_name} into {self.parent_pipeline_name}'\n",
    "                    >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary'\n",
    "                    >> beam.Map(reshapeCoGroupToFlatDict)\n",
    "        )\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', 'regionid', 'territories', 'regionid')\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef37a59",
   "metadata": {},
   "source": [
    "## BeamSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker images\n",
    "#! docker pull apache/beam_java11_sdk \n",
    "#! docker pull apache/beam_java8_sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc93081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile beamsql1.py\n",
    "# This code is not running in the notebook\n",
    "# This example just uses a basic Row object\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'Create Parent' >> beam.Create([(1, 'One'), (2, 'Two')])\n",
    "              | 'Map Parent' >> beam.Map(lambda x : beam.Row(parent_id = x[0], parent_name = x[1]))\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'Create Child' >> beam.Create([('Uno', 1), ('Due', 2), ('Eins', 1), ('Una', 1), ('Dos', 2)])\n",
    "              | 'Map Child' >> beam.Map(lambda x : beam.Row(child_name = x[0], parent_id = x[1]))\n",
    "    )\n",
    "    \n",
    "    result = ( {'parent': parent, 'child' : child} \n",
    "         | SqlTransform(\"\"\"\n",
    "             SELECT p.parent_id, p.parent_name, c.child_name \n",
    "             FROM parent as p \n",
    "             INNER JOIN child as c ON p.parent_id = c.parent_id\n",
    "             \"\"\")\n",
    "        | 'Map Join' >> beam.Map(lambda x : f'{x.parent_id} {x.parent_name} {x.child_name}')\n",
    "        | 'Print Join' >> beam.Map(print)\n",
    "        )\n",
    "#     result | beam.Map(..)\n",
    "#     result | beam.Map(..)\n",
    "#     parent | 'print parent' >> beam.Map(print)\n",
    "#     child  | 'print child' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile beamsql2.py\n",
    "# This code is not running in the notebook\n",
    "# This example uses a simple class to handle the schemas\n",
    "import apache_beam as beam\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "\n",
    "\n",
    "class Parent(typing.NamedTuple):\n",
    "    parent_id: int\n",
    "    parent_name: str\n",
    "beam.coders.registry.register_coder(Parent, beam.coders.RowCoder)\n",
    "\n",
    "class Child(typing.NamedTuple):\n",
    "    child_name: str\n",
    "    parent_id: int\n",
    "beam.coders.registry.register_coder(Child, beam.coders.RowCoder)\n",
    "        \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'Create Parent' >> beam.Create([(1, 'One'), (2, 'Two')])\n",
    "              | 'Map Parent' >> beam.Map(lambda x : Parent(parent_id = x[0], parent_name = x[1])).with_output_types(Parent)\n",
    "              | 'Map for Print' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'Create Child' >> beam.Create([('Uno', 1), ('Due', 2), ('Eins', 1), ('Una', 1), ('Dos', 2)])\n",
    "              | 'Map Child' >> beam.Map(lambda x : Child(child_name = x[0], parent_id = x[1])).with_output_types(Child)\n",
    "#               | 'SQL Child' >> SqlTransform(\"\"\"SELECT 10 * parent_id as parent_id, upper(child_name) as child_name from PCOLLECTION\"\"\")\n",
    "#               | 'Print Map' >> beam.Map(lambda x : f'{x.parent_id} = {x.child_name}')\n",
    "              | 'SQL Child' >> SqlTransform(\"\"\"SELECT parent_id, count(*) as cnt from PCOLLECTION GROUP BY parent_id\"\"\")\n",
    "              | 'Map for Print' >> beam.Map(lambda x : f'{x.parent_id} = {x.cnt}')\n",
    "              | 'Print SQL' >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391af3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile beamsql3.py\n",
    "# This code is not running in the notebook\n",
    "# This example is like example 2 but for a real file \n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "\n",
    "coders.registry.register_coder(Territory, coders.RowCoder)\n",
    "        \n",
    "@beam.typehints.with_output_types(Territory)\n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname.title(), int(regionid))\n",
    "\n",
    "        \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "#                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass()).with_output_types(Territory)\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                    | 'Map Territories for Print' >> beam.Map(lambda x : f'{x.regionid} - {x.cnt}')\n",
    "                    | 'Print SQL' >> beam.Map(print)\n",
    "                    )\n",
    "    \n",
    "#https://www.youtube.com/watch?v=zx4p-UNSmrA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                    )\n",
    "\n",
    "                    territories | 'Write to BQ' >> beam.WriteToBQ()\n",
    "                    territories | 'Write TO File' >> beam.WriteToText()\n",
    "#    p.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                  )\n",
    "                  \n",
    "                (territories | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                            | 'Write to BQ' >> beam.WriteToBQ()\n",
    "                )\n",
    "                (territories | 'SQL Territories2' >> SqlTransform(\"\"\"SELECT territoryid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                            | 'Write to BQ' >> beam.WriteToBQ()\n",
    "                    territories | 'Write TO File' >> beam.WriteToText()\n",
    "                )\n",
    "#    p.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c49f9",
   "metadata": {},
   "source": [
    "## DoFn Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a46bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def setup(self):\n",
    "        self.lookup = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        print('setup')\n",
    "        \n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "        \n",
    "    def process(self, element, uppercase = 0):\n",
    "        #lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = self.lookup.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), uppercase = 1 ) \n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        print('init')\n",
    "        #self.lookup = {1:'north', 2:'south', 3:'east', 4:'west'}\n",
    "        #self.init_semaphore = False\n",
    "\n",
    "    def called_once(self, lookuptable):\n",
    "        print('called_once')\n",
    "        self.lookup = { e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        self.init_semaphore = False\n",
    "\n",
    "    def setup(self):\n",
    "        print('setup')\n",
    "        self.init_semaphore = True\n",
    "        #self.lookup = {1:'NORTH', 2:'South', 3:'East', 4:'West'}\n",
    "\n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'north'}, {'regionid':2, 'regionname':'south'}]):\n",
    "        if self.init_semaphore:\n",
    "            self.called_once(lookuptable)\n",
    "        territoryid, territoryname, regionid = element\n",
    "        yield(territoryid, territoryname, regionid, self.lookup.get(regionid, 'No Region'))\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "        del self.init_semaphore\n",
    "                \n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Spare Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23638a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c00965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d0733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ef89c28",
   "metadata": {},
   "source": [
    "## Code below is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "#from collections import namedtuple\n",
    "from typing import NamedTuple\n",
    "from apache_beam import coders\n",
    "\n",
    "\n",
    "RegionSchema = namedtuple(\"RegionSchema\", (\"regionid\", \"regionname\"))\n",
    "#coders.registry.register_coder(RegionSchema, coders.RowCoder)\n",
    "class RegionSplitSchema(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield RegionSchema(int(regionid), regionname.title())\n",
    "\n",
    "#TerritorySchema = namedtuple(\"TerritorySchema\", (\"territoryid\", \"territoryname\", \"regionid\"))\n",
    "TerritorySchema = NamedTuple(\"TerritorySchema\", [(\"territoryid\", int), (\"territoryname\", str), (\"regionid\", int)])\n",
    "coders.registry.register_coder(TerritorySchema, coders.RowCoder)\n",
    "class TerritorySplitSchema(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield TerritorySchema(int(territoryid), territoryname.title(), int(regionid))\n",
    "\n",
    "        \n",
    "# class TerritorySplitNamedTuple(beam.DoFn):\n",
    "#     def process(self, element):\n",
    "#         territoryid, territoryname, regionid = element.split(',')\n",
    "#         yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "print('Start')\n",
    "with beam.Pipeline() as p:\n",
    "#     regions = (\n",
    "#               p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "#                 | 'Split Regions' >> beam.ParDo(RegionSplitSchema()).with_output_types(RegionSchema)\n",
    "#               )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplitSchema()) #.with_output_types(TerritorySchema)\n",
    "#                    | beam.Map(lambda x: TerritorySchema(x.territoryid, x.territoryname, x.regionid)).with_output_types(TerritorySchema)\n",
    "#         Map(lambda x: PythonSchema(f_int32=x)).with_output_types(PythonSchema)\n",
    "#                     | 'Apply Territories Schema' >> beam.Map(lambda x : beam.Row(territoryid = int(x.territoryid)\n",
    "#                                                                                  , territoryname = str(x.territoryname)\n",
    "#                                                                                  , regionid = int(x.regionid)))\n",
    "#                    | 'Convert to Dictionary' >> beam.Map(lambda row : {\"regionid\" : row.regionid, \"territoryid\" : row.territoryid, \"territoryname\" : row.territoryname})\n",
    "#                     | SqlTransform(\"\"\"\n",
    "#                         SELECT regionid, territoryname as name, territoryid \n",
    "#                         FROM PCOLLECTION\n",
    "#                         \"\"\")\n",
    "#                    | 'Split Territories' >> beam.ParDo(TerritorySplitSchema()).with_output_types(TerritorySchema)\n",
    "#                     | SqlTransform(\"\"\"\n",
    "#                         SELECT regionid, count(*) as territories\n",
    "#                         FROM PCOLLECTION\n",
    "#                         GROUP BY regionID\n",
    "#                         ORDER BY territories DESC\n",
    "#                         \"\"\")\n",
    "#                    | 'Convert to dictionary' >> beam.Map(lambda row : {\"regionid\": row.regionid, \"territories\": row.territories})\n",
    "                    \n",
    "#             })\n",
    "                  )\n",
    "\n",
    "#     regions | 'Print regions' >> beam.Map(print)\n",
    "    territories | 'Print territories' >> beam.Map(print)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c44a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with beam.Pipeline() as pipeline:\n",
    "#     _ = (\n",
    "#         pipeline\n",
    "#         | beam.io.ReadFromPubSub(\n",
    "#             topic='projects/pubsub-public-data/topics/taxirides-realtime',\n",
    "#             timestamp_attribute=\"ts\").with_output_types(bytes)\n",
    "#         | \"Parse JSON payload\" >> beam.Map(json.loads)\n",
    "#         # Use beam.Row to create a schema-aware PCollection\n",
    "#         | \"Create beam Row\" >> beam.Map(\n",
    "#             lambda x: beam.Row(\n",
    "#                 ride_status=str(x['ride_status']),\n",
    "#                 passenger_count=int(x['passenger_count'])))\n",
    "#         # SqlTransform will computes result within an existing window\n",
    "#         | \"15s fixed windows\" >> beam.WindowInto(beam.window.FixedWindows(15))\n",
    "#         # Aggregate drop offs and pick ups that occur within each 15s window\n",
    "#         | SqlTransform(\n",
    "#             \"\"\"\n",
    "#              SELECT\n",
    "#                ride_status,\n",
    "#                COUNT(*) AS num_rides,\n",
    "#                SUM(passenger_count) AS total_passengers\n",
    "#              FROM PCOLLECTION\n",
    "#              WHERE NOT ride_status = 'enroute'\n",
    "#              GROUP BY ride_status\"\"\")\n",
    "#         # SqlTransform yields python objects with attributes corresponding to\n",
    "#         # the outputs of the query.\n",
    "#         # Collect those attributes, as well as window information, into a dict\n",
    "#         | \"Assemble Dictionary\" >> beam.Map(\n",
    "#             lambda row,\n",
    "#             window=beam.DoFn.WindowParam: {\n",
    "#                 \"ride_status\": row.ride_status,\n",
    "#                 \"num_rides\": row.num_rides,\n",
    "#                 \"total_passengers\": row.total_passengers,\n",
    "#                 \"window_start\": window.start.to_rfc3339(),\n",
    "#                 \"window_end\": window.end.to_rfc3339()\n",
    "#             })\n",
    "#         | \"Convert to JSON\" >> beam.Map(json.dumps)\n",
    "#         | \"UTF-8 encode\" >> beam.Map(lambda s: s.encode(\"utf-8\"))\n",
    "#         | beam.Map(print)\n",
    "#         #| beam.io.WriteToPubSub(topic=output_topic))\n",
    "#     )\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   logging.getLogger().setLevel(logging.INFO)\n",
    "#   import argparse\n",
    "\n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.add_argument(\n",
    "#       '--output_topic',\n",
    "#       dest='output_topic',\n",
    "#       required=True,\n",
    "#       help=(\n",
    "#           'Cloud PubSub topic to write to (e.g. '\n",
    "#           'projects/my-project/topics/my-topic), must be created prior to '\n",
    "#           'running the pipeline.'))\n",
    "#   known_args, pipeline_args = parser.parse_known_args()\n",
    "\n",
    "#   run(known_args.output_topic, pipeline_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb002a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e57c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]\n",
    "lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a2485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsIter, AsSingleton, AsList, AsDict\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.io import ReadFromAvro, WriteToAvro\n",
    "from collections import namedtuple\n",
    "from apache_beam import coders\n",
    "from apache_beam.typehints.decorators import with_output_types\n",
    "\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, regionid, regionname):\n",
    "        self.regionid = regionid\n",
    "        self.regionname = regionname\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.regionid}|{self.regionname}'\n",
    "\n",
    "#     def encode(self, o):\n",
    "#         \"\"\"Encode to bytes with a trace that coder was used.\"\"\"\n",
    "#         # Our encoding prepends an 'x:' prefix.\n",
    "#         return b'x:%s' % o.encode('utf-8')\n",
    "\n",
    "#     def decode(self, s):\n",
    "#         # To decode, we strip off the prepended 'x:' prefix.\n",
    "#         s = s.decode('utf-8')\n",
    "#         #assert s[0:2] == 'x:'\n",
    "#         params = s[0:2].split('|')\n",
    "#         return Region(*params)\n",
    "\n",
    "#     def is_deterministic(self):\n",
    "#         # Since coded Player objects are used as keys below with\n",
    "#         # beam.CombinePerKey(sum), we require that this coder is deterministic\n",
    "#         # (i.e., two equivalent instances of the classes are encoded into the same\n",
    "#         # byte string) in order to guarantee consistent results.\n",
    "#         return True\n",
    "    \n",
    "class RegionSplitClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield Region(int(regionid), regionname.title())\n",
    "\n",
    "# class RegionCoder(coders.Coder):\n",
    "#   \"\"\"A custom coder for the RegionSchema\"\"\"\n",
    "#   def encode(self, o):\n",
    "#     \"\"\"Encode to bytes with a trace that coder was used.\"\"\"\n",
    "#     # Our encoding prepends an 'x:' prefix.\n",
    "#     return b'x:%s' % o.encode('utf-8')\n",
    "\n",
    "#   def decode(self, s):\n",
    "#     # To decode, we strip off the prepended 'x:' prefix.\n",
    "#     s = s.decode('utf-8')\n",
    "#     #assert s[0:2] == 'x:'\n",
    "#     params = s[0:2].split('|')\n",
    "#     return Region(*params)\n",
    "\n",
    "#   def is_deterministic(self):\n",
    "#     # Since coded Player objects are used as keys below with\n",
    "#     # beam.CombinePerKey(sum), we require that this coder is deterministic\n",
    "#     # (i.e., two equivalent instances of the classes are encoded into the same\n",
    "#     # byte string) in order to guarantee consistent results.\n",
    "#     return True\n",
    "# coders.registry.register_coder(Region, RegionCoder)\n",
    "\n",
    "# @with_output_types(typing.Tuple[Region, int])\n",
    "# def get_regions(descriptor):\n",
    "#   name, points = descriptor.split(',')\n",
    "#   return Player(name), int(points)\n",
    "\n",
    "\n",
    "# RegionSchema = namedtuple(\"RegionSchema\", (\"regionid\", \"regionname\"))\n",
    "# class RegionSplitSchema(beam.DoFn):\n",
    "#     def process(self, element):\n",
    "#         regionid, regionname = element.split(',')\n",
    "#         yield RegionSchema(int(regionid), regionname.title())\n",
    "\n",
    "class RegionSplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "\n",
    "class TerritorySplit(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "#        yield (int(regionid), (territoryid, territoryname.title())) \n",
    "        \n",
    "                \n",
    "def lookup_region(left, right):\n",
    "    territoryid, territoryname, regionid = left\n",
    "    yield territoryid, territoryname, regionid\n",
    "#    yield (territoryid, territorynme, regionid, right.get(regionid, 'No Region'))\n",
    "\n",
    "\n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "#        yield element\n",
    "        territoryid, territoryname, regionid = element\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "# #        yield (int(regionid), (territoryid, territoryname.title())) \n",
    "\n",
    "\n",
    "# def dummy(element):\n",
    "#     return element\n",
    "# #     regionid = element[0]\n",
    "# #     territoryid, territoryname = element[1]\n",
    "# #     return (territoryid, territoryname, regionid)\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Split Regions' >> beam.ParDo(RegionSplitDict())\n",
    "          #| 'Split Regions' >> beam.ParDo(RegionSplitClass())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#     regions = {1:\"North\", 2:\"South\", 3:\"East\", 4:\"West\"}\n",
    "#     regions = p | 'Create Regions' >> beam.Create([(1, 'North'), (2, 'South')])\n",
    "\n",
    "    \n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplit())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    join = (\n",
    "        territories\n",
    "          #| 'Lookup Region' >> beam.Map(dummy)\n",
    "#          | 'Lookup Region' >> beam.Map(lookup_region, right = beam.pvalue.AsList(regions))\n",
    "#        | beam.ParDo(LookupRegion())\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e6d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "\n",
    "class UnnestCoGrouped(beam.DoFn):\n",
    "    def process(self, item, child_pipeline, parent_pipeline):\n",
    "        k, v = item\n",
    "        child_dict = v[child_pipeline]\n",
    "        parent_dict = v[parent_pipeline]\n",
    "        for child in child_dict:\n",
    "            try:\n",
    "                child.update(parent_dict[0])\n",
    "                yield child\n",
    "            except IndexError:\n",
    "                yield child\n",
    "\n",
    "class LeftJoin(beam.PTransform):\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def _format_as_common_key_tuple(child_dict, child_key):\n",
    "            return (child_dict[child_key], child_dict)\n",
    "\n",
    "        return ({\n",
    "                pipeline_name: pcol1 | f'Convert to ({self.parent_key} = {self.child_key}, object) for {pipeline_name}' \n",
    "                >> beam.Map(_format_as_common_key_tuple, self.child_key)\n",
    "                for (pipeline_name, pcol1) in pcols.items()}\n",
    "                | f'CoGroupByKey {pcols.keys()}' >> beam.CoGroupByKey()\n",
    "                | 'Unnest Cogrouped' >> beam.ParDo(UnnestCoGrouped(), self.child_pipeline_name, self.parent_pipeline_name)\n",
    "        )\n",
    "        \n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    leftjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', 'regionid', 'territories', 'regionid')\n",
    "    leftjoin | 'Print Left Join' >> beam.Map(print)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class NestJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_name, parent_key, child_name, child_key):\n",
    "        self.parent_name = parent_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_name = child_name\n",
    "        self.child_key = child_key\n",
    "\n",
    "    @staticmethod\n",
    "    def excludeKeysFromDict(d, keyset):\n",
    "        return {k:v for k,v in d.items() if k in set(d.keys()).difference(keyset)}\n",
    "        \n",
    "    @staticmethod\n",
    "    def reshapeToKV(item, key):\n",
    "        # pipeline object should be a dictionary\n",
    "        return (item[key], NestJoin.excludeKeysFromDict(item, {key}))\n",
    "\n",
    "    def reshapeCoGroupToDict(self, item):\n",
    "        ret = {self.parent_key: item[0]\n",
    "              , **item[1][self.parent_name][0]\n",
    "              , self.child_name: item[1][self.child_name]}\n",
    "        return ret\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        return (\n",
    "                {\n",
    "                self.parent_name : pcols[self.parent_name] | f'Convert {self.parent_name} to KV' \n",
    "                    >> beam.Map(self.reshapeToKV, self.parent_key)\n",
    "                ,self.child_name : pcols[self.child_name] | f'Convert {self.child_name} to KV'\n",
    "                    >> beam.Map(self.reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_name} into {self.parent_name}' >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary' >> beam.Map(self.reshapeCoGroupToDict)\n",
    "               )\n",
    "\n",
    "class LeftJoin(NestJoin):\n",
    "    '''\n",
    "    Overloads the reshapeCoGroupToDict method to flatten out all the children to produce a traditional JOIN result\n",
    "    '''\n",
    "    def reshapeCoGroupToDict(self, item):\n",
    "        ret = [{self.parent_key: item[0]\n",
    "              , **item[1][self.parent_name][0]\n",
    "              , **row}\n",
    "              for row in item[1][self.child_name]]\n",
    "        return ret\n",
    "\n",
    "    \n",
    "    \n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', 'regionid', 'territories', 'regionid')\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fa984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.34.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.34.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
